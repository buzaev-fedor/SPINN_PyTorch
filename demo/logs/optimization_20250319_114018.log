[2025-03-19 11:40:18] Starting optimization process for all algorithms
[2025-03-19 11:40:18] ----------------------------------------
[2025-03-19 11:40:18] Running jade algorithm
[2025-03-19 11:40:18] ----------------------------------------
[2025-03-19 11:40:18] Starting optimization with jade algorithm...
/home/user/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

==================================================
Starting PINN optimization for Klein-Gordon equation
==================================================

Configuration:
Algorithm: JADE
NC (collocation points): 1000
NI (initial points): 100
NB (boundary points): 100
NC_TEST (test points): 50
Random seed: 42
Total epochs: 10000
Number of trials: 150
Timeout: None

Results will be saved to: /home/user/SPINN_PyTorch/results/klein_gordon3d/pinn_clear/jade_20250319_114020
Run parameters saved to run_params.json

============================================================
Initializing JADE optimizer
============================================================
Algorithm description:
JADE (Adaptive Differential Evolution) - Адаптивный алгоритм дифференциальной эволюции. Автоматически адаптирует параметры мутации и скрещивания. Эффективен для непрерывной оптимизации и хорошо масштабируется.
------------------------------------------------------------
Configuration:
  Number of trials: 150
  Timeout: None
  Algorithm parameters: {'population_size': 32, 'c': 0.1, 'p': 0.05}
------------------------------------------------------------

Starting optimization process...

============================================================
Starting optimization process
============================================================
Parameter bounds:
  n_layers            : (2, 5)
  layer_size          : (16, 128)
  lr_adamw            : (0.0001, 0.01)
  scheduler_factor    : (0.1, 0.5)
  scheduler_patience  : (5, 20)
  scheduler_min_lr    : (1e-06, 0.0001)
  scheduler_T_max     : (50, 200)
  scheduler_eta_min   : (1e-06, 0.0001)
------------------------------------------------------------

Initializing JADE optimizer...

Starting optimization...

------------------------------------------------------------
Trial 1/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.68e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.91e-01
************************************************************

Trial 1 completed:
  Current error: 1.91e-01
  Best error so far: 1.91e-01

------------------------------------------------------------
Trial 2/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.48e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 80
      Eta min: 5.19e-05

Training for 10000 epochs...

Trial 2 completed:
  Current error: 5.68e-01
  Best error so far: 1.91e-01

------------------------------------------------------------
Trial 3/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 8.10e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 68
      Eta min: 5.00e-05

Training for 10000 epochs...

************************************************************
New best error found: 1.41e-01
************************************************************

Trial 3 completed:
  Current error: 1.41e-01
  Best error so far: 1.41e-01

------------------------------------------------------------
Trial 4/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.93e-03
  Scheduler: none

Training for 10000 epochs...

Trial 4 completed:
  Current error: 7.46e-01
  Best error so far: 1.41e-01

------------------------------------------------------------
Trial 5/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 3.63e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 61
      Eta min: 9.87e-05

Training for 10000 epochs...

Trial 5 completed:
  Current error: 6.26e-01
  Best error so far: 1.41e-01

------------------------------------------------------------
Trial 6/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 8.33e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 100
      Eta min: 7.29e-06

Training for 10000 epochs...

Trial 6 completed:
  Current error: 2.47e-01
  Best error so far: 1.41e-01

------------------------------------------------------------
Trial 7/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 7 completed:
  Current error: 3.37e-01
  Best error so far: 1.41e-01

------------------------------------------------------------
Trial 8/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 2.57e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 93
      Eta min: 1.70e-05

Training for 10000 epochs...

Trial 8 completed:
  Current error: 3.43e-01
  Best error so far: 1.41e-01

------------------------------------------------------------
Trial 9/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 5.44e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.31e-01
************************************************************

Trial 9 completed:
  Current error: 1.31e-01
  Best error so far: 1.31e-01

------------------------------------------------------------
Trial 10/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.44e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.16e-01
************************************************************

Trial 10 completed:
  Current error: 1.16e-01
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 11/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.08e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.21
      Patience: 19
      Min lr: 2.47e-05

Training for 10000 epochs...

Trial 11 completed:
  Current error: 1.00e+00
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 12/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 6.36e-03
  Scheduler: none

Training for 10000 epochs...

Trial 12 completed:
  Current error: 9.96e-01
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 13/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 1.83e-03
  Scheduler: none

Training for 10000 epochs...

Trial 13 completed:
  Current error: 8.39e-01
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 14/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.49e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.46
      Patience: 19
      Min lr: 6.37e-05

Training for 10000 epochs...

Trial 14 completed:
  Current error: 6.77e-01
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 15/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 9.00e-03
  Scheduler: none

Training for 10000 epochs...

Trial 15 completed:
  Current error: 9.91e-01
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 16/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 7.49e-03
  Scheduler: none

Training for 10000 epochs...

Trial 16 completed:
  Current error: 9.96e-01
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 17/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 5.08e-03
  Scheduler: none

Training for 10000 epochs...

Trial 17 completed:
  Current error: 1.87e-01
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 18/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.29e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 94
      Eta min: 3.91e-05

Training for 10000 epochs...

Trial 18 completed:
  Current error: 1.49e-01
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 19/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.06e-03
  Scheduler: none

Training for 10000 epochs...

Trial 19 completed:
  Current error: 1.89e-01
  Best error so far: 1.16e-01

------------------------------------------------------------
Trial 20/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 9.14e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.03e-01
************************************************************

Trial 20 completed:
  Current error: 1.03e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 21/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.47e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 173
      Eta min: 3.67e-05

Training for 10000 epochs...

Trial 21 completed:
  Current error: 1.05e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 22/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.36e-03
  Scheduler: none

Training for 10000 epochs...

Trial 22 completed:
  Current error: 2.14e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 23/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 6.99e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 132
      Eta min: 7.17e-05

Training for 10000 epochs...

Trial 23 completed:
  Current error: 1.63e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 24/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.55e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 57
      Eta min: 5.03e-06

Training for 10000 epochs...

Trial 24 completed:
  Current error: 1.57e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 25/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 4.40e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 106
      Eta min: 6.30e-05

Training for 10000 epochs...

Trial 25 completed:
  Current error: 9.97e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 26/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.90e-03
  Scheduler: none

Training for 10000 epochs...

Trial 26 completed:
  Current error: 1.85e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 27/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 2.80e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 6
      Min lr: 3.26e-05

Training for 10000 epochs...

Trial 27 completed:
  Current error: 1.00e+00
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 28/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 8.05e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 199
      Eta min: 4.18e-05

Training for 10000 epochs...

Trial 28 completed:
  Current error: 2.03e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 29/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 7.57e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.46
      Patience: 13
      Min lr: 8.28e-05

Training for 10000 epochs...

Trial 29 completed:
  Current error: 9.74e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 30/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 5.78e-03
  Scheduler: none

Training for 10000 epochs...

Trial 30 completed:
  Current error: 1.61e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 31/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.89e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 143
      Eta min: 1.10e-05

Training for 10000 epochs...

Trial 31 completed:
  Current error: 3.25e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 32/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.87e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 198
      Eta min: 7.56e-05

Training for 10000 epochs...

Trial 32 completed:
  Current error: 2.77e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 33/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: none

Training for 10000 epochs...

Trial 33 completed:
  Current error: 1.91e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 34/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.48e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 85
      Eta min: 5.40e-05

Training for 10000 epochs...

Trial 34 completed:
  Current error: 6.15e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 35/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 8.10e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 68
      Eta min: 5.00e-05

Training for 10000 epochs...

Trial 35 completed:
  Current error: 4.21e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 36/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 1.93e-03
  Scheduler: none

Training for 10000 epochs...

Trial 36 completed:
  Current error: 2.57e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 37/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 3.63e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 142
      Eta min: 1.00e-04

Training for 10000 epochs...

Trial 37 completed:
  Current error: 8.90e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 38/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.34e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 165
      Eta min: 8.85e-05

Training for 10000 epochs...

Trial 38 completed:
  Current error: 3.10e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 39/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.93e-03
  Scheduler: none

Training for 10000 epochs...

Trial 39 completed:
  Current error: 2.60e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 40/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 8.67e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 93
      Eta min: 1.70e-05

Training for 10000 epochs...

Trial 40 completed:
  Current error: 6.54e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 41/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 5.44e-03
  Scheduler: none

Training for 10000 epochs...

Trial 41 completed:
  Current error: 2.23e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 42/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 3.44e-03
  Scheduler: none

Training for 10000 epochs...

Trial 42 completed:
  Current error: 9.97e-01
  Best error so far: 1.03e-01

------------------------------------------------------------
Trial 43/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.87e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 72
      Eta min: 4.95e-05

Training for 10000 epochs...

************************************************************
New best error found: 9.72e-02
************************************************************

Trial 43 completed:
  Current error: 9.72e-02
  Best error so far: 9.72e-02

------------------------------------------------------------
Trial 44/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 7.57e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 7.86e-02
************************************************************

Trial 44 completed:
  Current error: 7.86e-02
  Best error so far: 7.86e-02

------------------------------------------------------------
Trial 45/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 1.83e-03
  Scheduler: none

Training for 10000 epochs...

Trial 45 completed:
  Current error: 1.89e-01
  Best error so far: 7.86e-02

------------------------------------------------------------
Trial 46/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 4.28e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 81
      Eta min: 4.37e-05

Training for 10000 epochs...

Trial 46 completed:
  Current error: 1.23e-01
  Best error so far: 7.86e-02

------------------------------------------------------------
Trial 47/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 9.00e-03
  Scheduler: none

Training for 10000 epochs...

Trial 47 completed:
  Current error: 9.97e-01
  Best error so far: 7.86e-02

------------------------------------------------------------
Trial 48/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 7.49e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 64
      Eta min: 3.74e-05

Training for 10000 epochs...

Trial 48 completed:
  Current error: 9.96e-01
  Best error so far: 7.86e-02

------------------------------------------------------------
Trial 49/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 5.08e-03
  Scheduler: none

Training for 10000 epochs...

Trial 49 completed:
  Current error: 2.71e-01
  Best error so far: 7.86e-02

------------------------------------------------------------
Trial 50/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.68e-03
  Scheduler: none

Training for 10000 epochs...

Trial 50 completed:
  Current error: 1.66e-01
  Best error so far: 7.86e-02

------------------------------------------------------------
Trial 51/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.97e-03
  Scheduler: none

Training for 10000 epochs...

Trial 51 completed:
  Current error: 1.74e-01
  Best error so far: 7.86e-02

------------------------------------------------------------
Trial 52/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 9.14e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 149
      Eta min: 1.57e-05

Training for 10000 epochs...

************************************************************
New best error found: 7.83e-02
************************************************************

Trial 52 completed:
  Current error: 7.83e-02
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 53/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.05e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 118
      Eta min: 1.00e-06

Training for 10000 epochs...

Trial 53 completed:
  Current error: 7.97e-02
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 54/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.36e-03
  Scheduler: none

Training for 10000 epochs...

Trial 54 completed:
  Current error: 2.99e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 55/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: none

Training for 10000 epochs...

Trial 55 completed:
  Current error: 4.06e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 56/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.49e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 57
      Eta min: 5.03e-06

Training for 10000 epochs...

Trial 56 completed:
  Current error: 2.01e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 57/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 8.60e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 130
      Eta min: 6.30e-05

Training for 10000 epochs...

Trial 57 completed:
  Current error: 9.96e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 58/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.54e-03
  Scheduler: none

Training for 10000 epochs...

Trial 58 completed:
  Current error: 1.98e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 59/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.03e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 164
      Eta min: 3.30e-06

Training for 10000 epochs...

Trial 59 completed:
  Current error: 8.91e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 60/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 7.05e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 157
      Eta min: 2.14e-05

Training for 10000 epochs...

Trial 60 completed:
  Current error: 2.24e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 61/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 8.15e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 19
      Min lr: 8.66e-05

Training for 10000 epochs...

Trial 61 completed:
  Current error: 9.78e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 62/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 5.66e-03
  Scheduler: none

Training for 10000 epochs...

Trial 62 completed:
  Current error: 1.01e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 63/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.89e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 147
      Eta min: 1.10e-05

Training for 10000 epochs...

Trial 63 completed:
  Current error: 1.03e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 64/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 9.87e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 198
      Eta min: 7.56e-05

Training for 10000 epochs...

Trial 64 completed:
  Current error: 9.94e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 65/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: none

Training for 10000 epochs...

Trial 65 completed:
  Current error: 1.91e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 66/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.44e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 80
      Eta min: 2.06e-05

Training for 10000 epochs...

Trial 66 completed:
  Current error: 1.84e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 67/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 7.29e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 68
      Eta min: 6.94e-05

Training for 10000 epochs...

Trial 67 completed:
  Current error: 1.53e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 68/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 3.92e-03
  Scheduler: none

Training for 10000 epochs...

Trial 68 completed:
  Current error: 1.34e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 69/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 4.62e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 61
      Eta min: 9.87e-05

Training for 10000 epochs...

Trial 69 completed:
  Current error: 2.04e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 70/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 8.33e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 100
      Eta min: 7.29e-06

Training for 10000 epochs...

Trial 70 completed:
  Current error: 1.65e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 71/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: none

Training for 10000 epochs...

Trial 71 completed:
  Current error: 1.40e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 72/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 2.57e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 131
      Eta min: 1.70e-05

Training for 10000 epochs...

Trial 72 completed:
  Current error: 4.78e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 73/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 6.08e-03
  Scheduler: none

Training for 10000 epochs...

Trial 73 completed:
  Current error: 3.85e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 74/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.44e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 105
      Eta min: 4.53e-05

Training for 10000 epochs...

Trial 74 completed:
  Current error: 1.38e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 75/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.90e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 96
      Eta min: 4.95e-05

Training for 10000 epochs...

Trial 75 completed:
  Current error: 1.63e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 76/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 7.57e-03
  Scheduler: none

Training for 10000 epochs...

Trial 76 completed:
  Current error: 7.86e-02
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 77/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.86e-03
  Scheduler: none

Training for 10000 epochs...

Trial 77 completed:
  Current error: 9.60e-02
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 78/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 4.48e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 81
      Eta min: 4.37e-05

Training for 10000 epochs...

Trial 78 completed:
  Current error: 1.00e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 79/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 9.00e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 134
      Eta min: 1.69e-05

Training for 10000 epochs...

Trial 79 completed:
  Current error: 9.97e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 80/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: none

Training for 10000 epochs...

Trial 80 completed:
  Current error: 1.20e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 81/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 7.22e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 114
      Eta min: 3.41e-06

Training for 10000 epochs...

Trial 81 completed:
  Current error: 1.25e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 82/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.00e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 128
      Eta min: 2.93e-05

Training for 10000 epochs...

Trial 82 completed:
  Current error: 1.10e-01
  Best error so far: 7.83e-02

------------------------------------------------------------
Trial 83/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.88e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 5.03e-02
************************************************************

Trial 83 completed:
  Current error: 5.03e-02
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 84/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 5.55e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 152
      Eta min: 1.57e-05

Training for 10000 epochs...

Trial 84 completed:
  Current error: 4.38e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 85/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.39e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 118
      Eta min: 1.00e-06

Training for 10000 epochs...

Trial 85 completed:
  Current error: 3.15e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 86/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.36e-03
  Scheduler: none

Training for 10000 epochs...

Trial 86 completed:
  Current error: 2.28e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 87/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 6.99e-03
  Scheduler: none

Training for 10000 epochs...

Trial 87 completed:
  Current error: 7.80e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 88/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.55e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 172
      Eta min: 5.03e-06

Training for 10000 epochs...

Trial 88 completed:
  Current error: 4.10e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 89/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.35e-03
  Scheduler: none

Training for 10000 epochs...

Trial 89 completed:
  Current error: 1.35e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 90/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.63e-03
  Scheduler: none

Training for 10000 epochs...

Trial 90 completed:
  Current error: 1.33e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 91/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.03e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 164
      Eta min: 2.59e-05

Training for 10000 epochs...

Trial 91 completed:
  Current error: 8.91e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 92/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 4.35e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 199
      Eta min: 1.10e-05

Training for 10000 epochs...

Trial 92 completed:
  Current error: 7.86e-02
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 93/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 7.57e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.46
      Patience: 11
      Min lr: 6.80e-05

Training for 10000 epochs...

Trial 93 completed:
  Current error: 1.00e+00
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 94/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: none

Training for 10000 epochs...

Trial 94 completed:
  Current error: 2.90e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 95/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.77e-03
  Scheduler: none

Training for 10000 epochs...

Trial 95 completed:
  Current error: 1.77e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 96/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.87e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 169
      Eta min: 7.56e-05

Training for 10000 epochs...

Trial 96 completed:
  Current error: 2.46e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 97/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.26e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 146
      Eta min: 2.20e-05

Training for 10000 epochs...

Trial 97 completed:
  Current error: 2.11e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 98/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 5.44e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 80
      Eta min: 3.01e-05

Training for 10000 epochs...

Trial 98 completed:
  Current error: 2.61e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 99/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 8.10e-03
  Scheduler: none

Training for 10000 epochs...

Trial 99 completed:
  Current error: 5.48e-02
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 100/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 5.74e-03
  Scheduler: none

Training for 10000 epochs...

Trial 100 completed:
  Current error: 9.93e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 101/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: none

Training for 10000 epochs...

Trial 101 completed:
  Current error: 2.11e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 102/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 8.33e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 113
      Eta min: 2.90e-05

Training for 10000 epochs...

Trial 102 completed:
  Current error: 1.65e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 103/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: none

Training for 10000 epochs...

Trial 103 completed:
  Current error: 1.40e-01
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 104/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [16, 16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 5.06e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 131
      Eta min: 4.61e-05

Training for 10000 epochs...

Trial 104 completed:
  Current error: 7.21e-02
  Best error so far: 5.03e-02

------------------------------------------------------------
Trial 105/150
------------------------------------------------------------
Current parameters:
  Architecture: