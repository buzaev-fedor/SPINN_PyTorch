[2025-03-11 09:44:55] Starting optimization process for all algorithms
[2025-03-11 09:44:55] ----------------------------------------
[2025-03-11 09:44:55] Running jade algorithm
[2025-03-11 09:44:55] ----------------------------------------
[2025-03-11 09:44:55] Starting optimization with jade algorithm...
/home/user/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

==================================================
Starting SPINN optimization for Klein-Gordon equation
==================================================

Configuration:
Algorithm: JADE
NC (collocation points): 1000
NI (initial points): 100
NB (boundary points): 100
NC_TEST (test points): 50
Random seed: 42
Total epochs: 10000
Number of trials: 150
Timeout: None

Results will be saved to: /home/user/SPINN_PyTorch/results/klein_gordon3d/spinn_clear/jade_20250311_094457
Run parameters saved to run_params.json

============================================================
Initializing JADE optimizer
============================================================
Algorithm description:
JADE (Adaptive Differential Evolution) - Адаптивный алгоритм дифференциальной эволюции. Автоматически адаптирует параметры мутации и скрещивания. Эффективен для непрерывной оптимизации и хорошо масштабируется.
------------------------------------------------------------
Configuration:
  Number of trials: 150
  Timeout: None
  Algorithm parameters: {'population_size': 32, 'c': 0.1, 'p': 0.05}
------------------------------------------------------------

Starting optimization process...

============================================================
Starting optimization process
============================================================
Parameter bounds:
  n_layers            : (2, 5)
  layer_size          : (16, 128)
  lr_adamw            : (0.0001, 0.01)
  scheduler_factor    : (0.1, 0.5)
  scheduler_patience  : (5, 20)
  scheduler_min_lr    : (1e-06, 0.0001)
  scheduler_T_max     : (50, 200)
  scheduler_eta_min   : (1e-06, 0.0001)
------------------------------------------------------------

Initializing JADE optimizer...

Starting optimization...

------------------------------------------------------------
Trial 1/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 59
      Eta min: 8.68e-05

Training for 10000 epochs...

************************************************************
New best error found: 2.12e-02
************************************************************

Trial 1 completed:
  Current error: 2.12e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 2/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: relu
  Optimizers:
    AdamW lr: 3.04e-04
  Scheduler: none

Training for 10000 epochs...

Trial 2 completed:
  Current error: 1.00e+00
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 3/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: silu
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 94
      Eta min: 3.73e-05

Training for 10000 epochs...

Trial 3 completed:
  Current error: 2.18e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 4/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [104, 104, 104]
    Activation: silu
  Optimizers:
    AdamW lr: 2.08e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 141
      Eta min: 1.79e-05

Training for 10000 epochs...

Trial 4 completed:
  Current error: 3.21e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 5/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [122, 122]
    Activation: silu
  Optimizers:
    AdamW lr: 9.66e-03
  Scheduler: none

Training for 10000 epochs...

Trial 5 completed:
  Current error: 1.79e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 6/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: relu
  Optimizers:
    AdamW lr: 4.40e-04
  Scheduler: none

Training for 10000 epochs...

Trial 6 completed:
  Current error: 9.98e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 7/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [37, 37, 37, 37]
    Activation: relu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: none

Training for 10000 epochs...

Trial 7 completed:
  Current error: 7.99e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 8/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [38, 38]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 174
      Eta min: 3.63e-05

Training for 10000 epochs...

Trial 8 completed:
  Current error: 1.26e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 9/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [77, 77, 77]
    Activation: relu
  Optimizers:
    AdamW lr: 1.50e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 166
      Eta min: 2.07e-05

Training for 10000 epochs...

Trial 9 completed:
  Current error: 9.99e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 10/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [107, 107]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.10e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.39
      Patience: 17
      Min lr: 8.33e-06

Training for 10000 epochs...

Trial 10 completed:
  Current error: 9.48e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 11/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [86, 86, 86, 86, 86]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.38e-03
  Scheduler: none

Training for 10000 epochs...

Trial 11 completed:
  Current error: 7.49e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 12/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [69, 69, 69, 69, 69]
    Activation: relu
  Optimizers:
    AdamW lr: 1.28e-03
  Scheduler: none

Training for 10000 epochs...

Trial 12 completed:
  Current error: 1.00e+00
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 13/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [64, 64, 64, 64]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.52e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 5.13e-05

Training for 10000 epochs...

Trial 13 completed:
  Current error: 4.74e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 14/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 93
      Eta min: 1.70e-05

Training for 10000 epochs...

Trial 14 completed:
  Current error: 2.55e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 15/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 184
      Eta min: 5.44e-05

Training for 10000 epochs...

Trial 15 completed:
  Current error: 2.57e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 16/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [116, 116, 116, 116]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.25e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.14
      Patience: 8
      Min lr: 4.33e-05

Training for 10000 epochs...

Trial 16 completed:
  Current error: 1.56e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 17/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [73, 73]
    Activation: silu
  Optimizers:
    AdamW lr: 4.23e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 191
      Eta min: 3.30e-05

Training for 10000 epochs...

************************************************************
New best error found: 1.76e-02
************************************************************

Trial 17 completed:
  Current error: 1.76e-02
  Best error so far: 1.76e-02

------------------------------------------------------------
Trial 18/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 19
      Min lr: 2.59e-05

Training for 10000 epochs...

Trial 18 completed:
  Current error: 4.64e-02
  Best error so far: 1.76e-02

------------------------------------------------------------
Trial 19/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [20, 20, 20]
    Activation: silu
  Optimizers:
    AdamW lr: 6.13e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.30
      Patience: 6
      Min lr: 2.86e-05

Training for 10000 epochs...

Trial 19 completed:
  Current error: 6.74e-01
  Best error so far: 1.76e-02

------------------------------------------------------------
Trial 20/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: silu
  Optimizers:
    AdamW lr: 9.86e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.20
      Patience: 15
      Min lr: 7.64e-05

Training for 10000 epochs...

Trial 20 completed:
  Current error: 2.28e-01
  Best error so far: 1.76e-02

------------------------------------------------------------
Trial 21/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: silu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 98
      Eta min: 1.95e-05

Training for 10000 epochs...

************************************************************
New best error found: 1.15e-02
************************************************************

Trial 21 completed:
  Current error: 1.15e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 22/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [82, 82]
    Activation: silu
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 147
      Eta min: 1.83e-05

Training for 10000 epochs...

Trial 22 completed:
  Current error: 2.10e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 23/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: silu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: none

Training for 10000 epochs...

Trial 23 completed:
  Current error: 2.32e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 24/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [90, 90, 90]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.19e-03
  Scheduler: none

Training for 10000 epochs...

Trial 24 completed:
  Current error: 2.40e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 25/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [87, 87, 87, 87, 87]
    Activation: silu
  Optimizers:
    AdamW lr: 3.46e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 183
      Eta min: 7.82e-05

Training for 10000 epochs...

Trial 25 completed:
  Current error: 2.16e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 26/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [25, 25, 25, 25]
    Activation: relu
  Optimizers:
    AdamW lr: 1.70e-03
  Scheduler: none

Training for 10000 epochs...

Trial 26 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 27/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 157
      Eta min: 2.45e-05

Training for 10000 epochs...

Trial 27 completed:
  Current error: 2.94e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 28/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [100, 100, 100]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.44
      Patience: 15
      Min lr: 5.73e-05

Training for 10000 epochs...

Trial 28 completed:
  Current error: 1.07e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 29/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [43, 43, 43]
    Activation: relu
  Optimizers:
    AdamW lr: 9.73e-03
  Scheduler: none

Training for 10000 epochs...

Trial 29 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 30/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [71, 71, 71, 71]
    Activation: relu
  Optimizers:
    AdamW lr: 2.03e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.39
      Patience: 9
      Min lr: 3.41e-06

Training for 10000 epochs...

Trial 30 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 31/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [123, 123, 123, 123, 123]
    Activation: relu
  Optimizers:
    AdamW lr: 9.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 114
      Eta min: 9.67e-05

Training for 10000 epochs...

Trial 31 completed:
  Current error: 9.98e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 32/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [112, 112, 112, 112, 112]
    Activation: relu
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 75
      Eta min: 5.61e-05

Training for 10000 epochs...

Trial 32 completed:
  Current error: 8.81e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 33/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [128, 128, 128]
    Activation: relu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.34
      Patience: 5
      Min lr: 1.64e-05

Training for 10000 epochs...

Trial 33 completed:
  Current error: 1.02e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 34/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [95, 95]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.90e-03
  Scheduler: none

Training for 10000 epochs...

Trial 34 completed:
  Current error: 5.14e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 35/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: silu
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: none

Training for 10000 epochs...

Trial 35 completed:
  Current error: 1.67e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 36/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [128, 128, 128]
    Activation: silu
  Optimizers:
    AdamW lr: 3.56e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 14
      Min lr: 5.60e-06

Training for 10000 epochs...

Trial 36 completed:
  Current error: 2.41e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 37/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [122, 122]
    Activation: silu
  Optimizers:
    AdamW lr: 8.64e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 153
      Eta min: 4.46e-05

Training for 10000 epochs...

Trial 37 completed:
  Current error: 3.75e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 38/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.40e-04
  Scheduler: none

Training for 10000 epochs...

Trial 38 completed:
  Current error: 1.19e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 39/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [78, 78, 78]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 19
      Min lr: 8.96e-05

Training for 10000 epochs...

Trial 39 completed:
  Current error: 1.48e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 40/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [20, 20, 20]
    Activation: relu
  Optimizers:
    AdamW lr: 3.77e-03
  Scheduler: none

Training for 10000 epochs...

Trial 40 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 41/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [77, 77, 77]
    Activation: relu
  Optimizers:
    AdamW lr: 4.60e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.42
      Patience: 7
      Min lr: 9.87e-05

Training for 10000 epochs...

Trial 41 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 42/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [80, 80]
    Activation: relu
  Optimizers:
    AdamW lr: 5.29e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.42
      Patience: 10
      Min lr: 5.01e-05

Training for 10000 epochs...

Trial 42 completed:
  Current error: 1.04e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 43/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [86, 86, 86, 86, 86]
    Activation: silu
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 139
      Eta min: 6.41e-05

Training for 10000 epochs...

Trial 43 completed:
  Current error: 1.92e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 44/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [69, 69, 69, 69]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.40e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 63
      Eta min: 2.39e-05

Training for 10000 epochs...

Trial 44 completed:
  Current error: 1.70e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 45/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [64, 64, 64, 64]
    Activation: relu
  Optimizers:
    AdamW lr: 3.52e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 93
      Eta min: 5.13e-05

Training for 10000 epochs...

Trial 45 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 46/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [44, 44, 44]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.38e-03
  Scheduler: none

Training for 10000 epochs...

Trial 46 completed:
  Current error: 2.48e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 47/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: silu
  Optimizers:
    AdamW lr: 5.06e-03
  Scheduler: none

Training for 10000 epochs...

Trial 47 completed:
  Current error: 2.36e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 48/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [116, 116, 116, 116]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.17e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.14
      Patience: 8
      Min lr: 4.33e-05

Training for 10000 epochs...

Trial 48 completed:
  Current error: 1.77e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 49/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [73, 73]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.66e-03
  Scheduler: none

Training for 10000 epochs...

Trial 49 completed:
  Current error: 9.76e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 50/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [100, 100, 100, 100]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.70e-03
  Scheduler: none

Training for 10000 epochs...

Trial 50 completed:
  Current error: 2.05e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 51/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [20, 20, 20]
    Activation: relu
  Optimizers:
    AdamW lr: 8.22e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 157
      Eta min: 1.24e-05

Training for 10000 epochs...

Trial 51 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 52/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [58, 58]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.86e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 155
      Eta min: 7.31e-05

Training for 10000 epochs...

Trial 52 completed:
  Current error: 6.70e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 53/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: none

Training for 10000 epochs...

Trial 53 completed:
  Current error: 2.23e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 54/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [82, 82]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: none

Training for 10000 epochs...

Trial 54 completed:
  Current error: 7.73e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 55/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: none

Training for 10000 epochs...

Trial 55 completed:
  Current error: 2.25e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 56/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [90, 90, 90]
    Activation: relu
  Optimizers:
    AdamW lr: 8.01e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 50
      Eta min: 6.92e-05

Training for 10000 epochs...

Trial 56 completed:
  Current error: 1.01e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 57/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [44, 44, 44, 44]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.99e-03
  Scheduler: none

Training for 10000 epochs...

Trial 57 completed:
  Current error: 1.67e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 58/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [86, 86, 86, 86]
    Activation: silu
  Optimizers:
    AdamW lr: 1.60e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.43
      Patience: 14
      Min lr: 6.24e-05

Training for 10000 epochs...

Trial 58 completed:
  Current error: 6.88e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 59/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [39, 39]
    Activation: silu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: none

Training for 10000 epochs...

Trial 59 completed:
  Current error: 1.89e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 60/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [81, 81, 81]
    Activation: silu
  Optimizers:
    AdamW lr: 6.53e-03
  Scheduler: none

Training for 10000 epochs...

Trial 60 completed:
  Current error: 1.36e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 61/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [43, 43, 43, 43]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.21e-03
  Scheduler: none

Training for 10000 epochs...

Trial 61 completed:
  Current error: 3.60e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 62/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [82, 82, 82, 82]
    Activation: relu
  Optimizers:
    AdamW lr: 2.03e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 147
      Eta min: 1.85e-05

Training for 10000 epochs...

Trial 62 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 63/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [123, 123, 123, 123]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.87e-03
  Scheduler: none

Training for 10000 epochs...

Trial 63 completed:
  Current error: 2.73e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 64/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [100, 100, 100, 100]
    Activation: relu
  Optimizers:
    AdamW lr: 3.51e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 50
      Eta min: 5.87e-05

Training for 10000 epochs...

Trial 64 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 65/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [111, 111, 111]
    Activation: silu
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: none

Training for 10000 epochs...

Trial 65 completed:
  Current error: 2.32e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 66/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [95, 95]
    Activation: silu
  Optimizers:
    AdamW lr: 1.90e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 1.00e-06

Training for 10000 epochs...

Trial 66 completed:
  Current error: 1.79e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 67/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [98, 98, 98, 98]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.95e-03
  Scheduler: none

Training for 10000 epochs...

Trial 67 completed:
  Current error: 1.79e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 68/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [111, 111, 111]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.08e-03
  Scheduler: none

Training for 10000 epochs...

Trial 68 completed:
  Current error: 4.18e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 69/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [88, 88, 88]
    Activation: silu
  Optimizers:
    AdamW lr: 5.93e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 117
      Eta min: 4.61e-05

Training for 10000 epochs...

Trial 69 completed:
  Current error: 2.46e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 70/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [61, 61]
    Activation: relu
  Optimizers:
    AdamW lr: 5.24e-03
  Scheduler: none

Training for 10000 epochs...

Trial 70 completed:
  Current error: 1.01e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 71/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [78, 78, 78]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: cosine_annealing
    Parameters:
      T_max: 83
      Eta min: 9.23e-05

Training for 10000 epochs...

Trial 71 completed:
  Current error: 2.53e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 72/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [63, 63, 63]
    Activation: relu
  Optimizers:
    AdamW lr: 3.86e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 89
      Eta min: 3.63e-05

Training for 10000 epochs...

Trial 72 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 73/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [77, 77, 77]
    Activation: relu
  Optimizers:
    AdamW lr: 4.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 166
      Eta min: 2.07e-05

Training for 10000 epochs...

Trial 73 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 74/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [99, 99]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.10e-03
  Scheduler: none

Training for 10000 epochs...

Trial 74 completed:
  Current error: 3.32e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 75/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [86, 86, 86, 86]
    Activation: relu
  Optimizers:
    AdamW lr: 8.39e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.13
      Patience: 12
      Min lr: 9.97e-05

Training for 10000 epochs...

Trial 75 completed:
  Current error: 1.02e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 76/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [89, 89, 89, 89]
    Activation: tanh
  Optimizers:
    AdamW lr: 2.40e-03
  Scheduler: none

Training for 10000 epochs...

Trial 76 completed:
  Current error: 4.04e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 77/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 86
      Eta min: 5.13e-05

Training for 10000 epochs...

Trial 77 completed:
  Current error: 3.86e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 78/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [44, 44, 44]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.38e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.26
      Patience: 5
      Min lr: 6.19e-05

Training for 10000 epochs...

Trial 78 completed:
  Current error: 1.07e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 79/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.06e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.33
      Patience: 5
      Min lr: 7.73e-05

Training for 10000 epochs...

Trial 79 completed:
  Current error: 8.88e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 80/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [75, 75, 75, 75]
    Activation: relu
  Optimizers:
    AdamW lr: 3.74e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.10
      Patience: 12
      Min lr: 4.33e-05

Training for 10000 epochs...

Trial 80 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 81/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: silu
  Optimizers:
    AdamW lr: 6.21e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 95
      Eta min: 5.74e-05

Training for 10000 epochs...

Trial 81 completed:
  Current error: 2.93e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 82/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [100, 100, 100, 100]
    Activation: silu
  Optimizers:
    AdamW lr: 3.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 11
      Min lr: 2.59e-05

Training for 10000 epochs...

Trial 82 completed:
  Current error: 8.89e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 83/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [54, 54, 54]
    Activation: silu
  Optimizers:
    AdamW lr: 8.21e-03
  Scheduler: none

Training for 10000 epochs...

Trial 83 completed:
  Current error: 1.59e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 84/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [58, 58, 58]
    Activation: relu
  Optimizers:
    AdamW lr: 9.86e-03
  Scheduler: none

Training for 10000 epochs...

Trial 84 completed:
  Current error: 1.04e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 85/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: relu
  Optimizers:
    AdamW lr: 5.55e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 111
      Eta min: 1.68e-05

Training for 10000 epochs...

Trial 85 completed:
  Current error: 1.01e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 86/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [82, 82, 82]
    Activation: relu
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 147
      Eta min: 1.83e-05

Training for 10000 epochs...

Trial 86 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 87/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [59, 59, 59]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.16
      Patience: 10
      Min lr: 2.91e-05

Training for 10000 epochs...

Trial 87 completed:
  Current error: 5.98e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 88/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [113, 113, 113]
    Activation: relu
  Optimizers:
    AdamW lr: 8.19e-03
  Scheduler: none

Training for 10000 epochs...

Trial 88 completed:
  Current error: 9.98e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 89/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [118, 118, 118]
    Activation: silu
  Optimizers:
    AdamW lr: 5.89e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.27
      Patience: 12
      Min lr: 7.48e-05

Training for 10000 epochs...

Trial 89 completed:
  Current error: 1.38e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 90/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [86, 86, 86, 86]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.22e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.28
      Patience: 14
      Min lr: 8.20e-05

Training for 10000 epochs...

Trial 90 completed:
  Current error: 3.74e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 91/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [59, 59]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.37e-03
  Scheduler: none

Training for 10000 epochs...

Trial 91 completed:
  Current error: 8.75e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 92/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [74, 74, 74]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.24e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 94
      Eta min: 4.02e-05

Training for 10000 epochs...

Trial 92 completed:
  Current error: 9.44e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 93/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [43, 43, 43, 43]
    Activation: relu
  Optimizers:
    AdamW lr: 4.21e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 161
      Eta min: 5.66e-05

Training for 10000 epochs...

Trial 93 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 94/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [76, 76, 76]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.81e-03
  Scheduler: none

Training for 10000 epochs...

Trial 94 completed:
  Current error: 3.98e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 95/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [105, 105, 105, 105]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.87e-03
  Scheduler: none

Training for 10000 epochs...

Trial 95 completed:
  Current error: 5.88e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 96/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [116, 116, 116, 116, 116]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: none

Training for 10000 epochs...

Trial 96 completed:
  Current error: 5.11e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 97/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.75e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.36
      Patience: 7
      Min lr: 6.14e-05

Training for 10000 epochs...

Trial 97 completed:
  Current error: 1.88e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 98/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [95, 95]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.90e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 1.00e-06

Training for 10000 epochs...

Trial 98 completed:
  Current error: 2.14e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 99/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: relu
  Optimizers:
    AdamW lr: 2.06e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.22
      Patience: 12
      Min lr: 6.74e-05

Training for 10000 epochs...

Trial 99 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 100/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [104, 104, 104]
    Activation: relu
  Optimizers:
    AdamW lr: 5.11e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 94
      Eta min: 1.79e-05

Training for 10000 epochs...

Trial 100 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 101/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [127, 127, 127]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.93e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 5
      Min lr: 9.75e-05

Training for 10000 epochs...

Trial 101 completed:
  Current error: 2.51e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 102/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [73, 73, 73]
    Activation: silu
  Optimizers:
    AdamW lr: 4.40e-04
  Scheduler: none

Training for 10000 epochs...

Trial 102 completed:
  Current error: 5.08e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 103/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [78, 78, 78]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.46e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 19
      Min lr: 8.93e-05

Training for 10000 epochs...

Trial 103 completed:
  Current error: 3.82e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 104/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [42, 42, 42, 42]
    Activation: silu
  Optimizers:
    AdamW lr: 4.36e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 169
      Eta min: 6.69e-05

Training for 10000 epochs...

Trial 104 completed:
  Current error: 2.90e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 105/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [77, 77, 77]
    Activation: relu
  Optimizers:
    AdamW lr: 4.60e-03
  Scheduler: none

Training for 10000 epochs...

Trial 105 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 106/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [99, 99]
    Activation: relu
  Optimizers:
    AdamW lr: 5.22e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 106
      Eta min: 6.01e-06

Training for 10000 epochs...

Trial 106 completed:
  Current error: 1.01e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 107/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [86, 86, 86, 86]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 139
      Eta min: 6.41e-05

Training for 10000 epochs...

Trial 107 completed:
  Current error: 1.16e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 108/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [67, 67, 67, 67]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.36e-03
  Scheduler: none

Training for 10000 epochs...

Trial 108 completed:
  Current error: 3.47e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 109/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [88, 88, 88, 88]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.00e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 86
      Eta min: 5.13e-05

Training for 10000 epochs...

Trial 109 completed:
  Current error: 6.06e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 110/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [69, 69, 69]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.54e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 118
      Eta min: 3.80e-05

Training for 10000 epochs...

Trial 110 completed:
  Current error: 6.02e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 111/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.06e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 184
      Eta min: 1.80e-05

Training for 10000 epochs...

Trial 111 completed:
  Current error: 2.14e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 112/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [88, 88, 88]
    Activation: silu
  Optimizers:
    AdamW lr: 7.74e-03
  Scheduler: none

Training for 10000 epochs...

Trial 112 completed:
  Current error: 2.66e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 113/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [73, 73, 73, 73]
    Activation: silu
  Optimizers:
    AdamW lr: 4.23e-03
  Scheduler: none

Training for 10000 epochs...

Trial 113 completed:
  Current error: 1.17e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 114/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [96, 96, 96, 96]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.70e-03
  Scheduler: none

Training for 10000 epochs...

Trial 114 completed:
  Current error: 1.59e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 115/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [61, 61, 61]
    Activation: silu
  Optimizers:
    AdamW lr: 6.09e-03
  Scheduler: none

Training for 10000 epochs...

Trial 115 completed:
  Current error: 2.76e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 116/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [56, 56, 56]
    Activation: relu
  Optimizers:
    AdamW lr: 7.31e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 147
      Eta min: 7.31e-05

Training for 10000 epochs...

Trial 116 completed:
  Current error: 1.01e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 117/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [108, 108, 108]
    Activation: relu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 148
      Eta min: 1.95e-05

Training for 10000 epochs...

Trial 117 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 118/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [82, 82, 82]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 11
      Min lr: 9.95e-06

Training for 10000 epochs...

Trial 118 completed:
  Current error: 8.81e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 119/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [63, 63, 63, 63]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 189
      Eta min: 4.42e-05

Training for 10000 epochs...

Trial 119 completed:
  Current error: 2.65e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 120/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [95, 95, 95]
    Activation: silu
  Optimizers:
    AdamW lr: 8.19e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.24
      Patience: 13
      Min lr: 2.49e-05

Training for 10000 epochs...

Trial 120 completed:
  Current error: 1.89e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 121/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [50, 50, 50, 50]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.99e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 200
      Eta min: 1.83e-06

Training for 10000 epochs...

Trial 121 completed:
  Current error: 2.55e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 122/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [114, 114, 114, 114, 114]
    Activation: silu
  Optimizers:
    AdamW lr: 1.60e-03
  Scheduler: none

Training for 10000 epochs...

Trial 122 completed:
  Current error: 2.49e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 123/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [62, 62, 62]
    Activation: relu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.35
      Patience: 16
      Min lr: 7.73e-05

Training for 10000 epochs...

Trial 123 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 124/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [78, 78, 78]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.07e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 121
      Eta min: 4.02e-05

Training for 10000 epochs...

Trial 124 completed:
  Current error: 3.69e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 125/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [90, 90, 90, 90]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.21e-03
  Scheduler: none

Training for 10000 epochs...

Trial 125 completed:
  Current error: 8.69e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 126/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [84, 84, 84]
    Activation: silu
  Optimizers:
    AdamW lr: 3.81e-03
  Scheduler: none

Training for 10000 epochs...

Trial 126 completed:
  Current error: 2.16e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 127/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [94, 94, 94, 94]
    Activation: silu
  Optimizers:
    AdamW lr: 7.28e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.27
      Patience: 5
      Min lr: 1.00e-04

Training for 10000 epochs...

Trial 127 completed:
  Current error: 8.33e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 128/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [116, 116, 116, 116, 116]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.27
      Patience: 18
      Min lr: 3.24e-05

Training for 10000 epochs...

Trial 128 completed:
  Current error: 5.12e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 129/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: silu
  Optimizers:
    AdamW lr: 8.19e-03
  Scheduler: none

Training for 10000 epochs...

Trial 129 completed:
  Current error: 3.17e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 130/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [108, 108]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.87e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 61
      Eta min: 1.00e-06

Training for 10000 epochs...

Trial 130 completed:
  Current error: 4.99e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 131/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.34e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.22
      Patience: 13
      Min lr: 4.24e-05

Training for 10000 epochs...

Trial 131 completed:
  Current error: 2.20e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 132/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [104, 104, 104]
    Activation: silu
  Optimizers:
    AdamW lr: 2.65e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 15
      Min lr: 3.37e-05

Training for 10000 epochs...

Trial 132 completed:
  Current error: 9.07e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 133/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [88, 88, 88]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.78e-03
  Scheduler: none

Training for 10000 epochs...

Trial 133 completed:
  Current error: 2.35e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 134/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [73, 73]
    Activation: relu
  Optimizers:
    AdamW lr: 2.55e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 59
      Eta min: 4.09e-05

Training for 10000 epochs...

Trial 134 completed:
  Current error: 1.02e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 135/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [78, 78]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 10
      Min lr: 8.96e-05

Training for 10000 epochs...

Trial 135 completed:
  Current error: 2.57e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 136/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [42, 42, 42, 42]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.36e-03
  Scheduler: none

Training for 10000 epochs...

Trial 136 completed:
  Current error: 4.39e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 137/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [77, 77, 77]
    Activation: silu
  Optimizers:
    AdamW lr: 4.60e-03
  Scheduler: none

Training for 10000 epochs...

Trial 137 completed:
  Current error: 1.65e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 138/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [68, 68]
    Activation: silu
  Optimizers:
    AdamW lr: 5.30e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 11
      Min lr: 2.42e-05

Training for 10000 epochs...

Trial 138 completed:
  Current error: 2.61e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 139/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [91, 91, 91, 91]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.42e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 139
      Eta min: 3.92e-05

Training for 10000 epochs...

Trial 139 completed:
  Current error: 3.00e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 140/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [69, 69, 69, 69, 69]
    Activation: silu
  Optimizers:
    AdamW lr: 1.32e-03
  Scheduler: none

Training for 10000 epochs...

Trial 140 completed:
  Current error: 3.02e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 141/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [64, 64]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.87e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 5
      Min lr: 6.40e-05

Training for 10000 epochs...

Trial 141 completed:
  Current error: 2.69e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 142/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [61, 61, 61]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.56e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.12
      Patience: 8
      Min lr: 1.87e-05

Training for 10000 epochs...

Trial 142 completed:
  Current error: 7.67e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 143/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: relu
  Optimizers:
    AdamW lr: 3.96e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.33
      Patience: 5
      Min lr: 1.00e-04

Training for 10000 epochs...

Trial 143 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 144/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [95, 95, 95]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.74e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.22
      Patience: 8
      Min lr: 4.63e-05

Training for 10000 epochs...

Trial 144 completed:
  Current error: 1.76e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 145/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [53, 53, 53, 53]
    Activation: silu
  Optimizers:
    AdamW lr: 4.23e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 183
      Eta min: 2.42e-05

Training for 10000 epochs...

Trial 145 completed:
  Current error: 3.14e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 146/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [75, 75, 75, 75]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.70e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 109
      Eta min: 2.31e-05

Training for 10000 epochs...

Trial 146 completed:
  Current error: 4.49e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 147/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [76, 76, 76]
    Activation: relu
  Optimizers:
    AdamW lr: 8.21e-03
  Scheduler: none

Training for 10000 epochs...

Trial 147 completed:
  Current error: 9.98e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 148/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [58, 58, 58, 58]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.86e-03
  Scheduler: none

Training for 10000 epochs...

Trial 148 completed:
  Current error: 2.48e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 149/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.16
      Patience: 6
      Min lr: 8.37e-05

Training for 10000 epochs...

Trial 149 completed:
  Current error: 1.71e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 150/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [82, 82]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 115
      Eta min: 1.00e-06

Training for 10000 epochs...

Trial 150 completed:
  Current error: 3.78e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 151/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [83, 83, 83, 83]
    Activation: relu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 118
      Eta min: 7.62e-05

Training for 10000 epochs...

Trial 151 completed:
  Current error: 1.03e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 152/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [90, 90, 90]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.19e-03
  Scheduler: none

Training for 10000 epochs...

Trial 152 completed:
  Current error: 2.83e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 153/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [44, 44, 44, 44]
    Activation: relu
  Optimizers:
    AdamW lr: 4.98e-03
  Scheduler: none

Training for 10000 epochs...

Trial 153 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 154/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [107, 107, 107, 107]
    Activation: silu
  Optimizers:
    AdamW lr: 1.60e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 50
      Eta min: 3.01e-05

Training for 10000 epochs...

Trial 154 completed:
  Current error: 3.23e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 155/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [39, 39]
    Activation: relu
  Optimizers:
    AdamW lr: 7.18e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.19
      Patience: 15
      Min lr: 1.92e-05

Training for 10000 epochs...

Trial 155 completed:
  Current error: 1.01e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 156/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [81, 81, 81]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.53e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 114
      Eta min: 4.02e-05

Training for 10000 epochs...

Trial 156 completed:
  Current error: 8.36e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 157/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [43, 43, 43]
    Activation: relu
  Optimizers:
    AdamW lr: 7.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 169
      Eta min: 5.66e-05

Training for 10000 epochs...

Trial 157 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 158/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [84, 84, 84]
    Activation: relu
  Optimizers:
    AdamW lr: 3.50e-03
  Scheduler: none

Training for 10000 epochs...

Trial 158 completed:
  Current error: 9.98e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 159/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [105, 105, 105, 105]
    Activation: silu
  Optimizers:
    AdamW lr: 8.87e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 6
      Min lr: 7.14e-05

Training for 10000 epochs...

Trial 159 completed:
  Current error: 1.27e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 160/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [124, 124, 124, 124, 124]
    Activation: relu
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.36
      Patience: 10
      Min lr: 6.24e-05

Training for 10000 epochs...

Trial 160 completed:
  Current error: 9.98e-01
  Best error so far: 1.15e-02

============================================================
Optimization completed successfully!
============================================================
Total time: 52046.40 seconds
Best objective value: 1.15e-02

Best parameters found:
------------------------------------------------------------
Architecture:
  Number of layers: 3
  Layer sizes: [87, 87, 87]
  Activation: gelu

Optimizers:
  AdamW learning rate: 6.37e-03

Scheduler type: cosine_annealing
  Parameters:
    T_max: 98
    Eta min: 1.95e-05
------------------------------------------------------------

Saving results to logger...
Traceback (most recent call last):
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1305, in <module>
    main(args.nc, args.ni, args.nb, args.nc_test, args.seed, args.epochs)
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1279, in main
    main_with_algorithm(args.algorithm, args.n_trials, args.timeout, algorithm_params=algorithm_params, **params)
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1134, in main_with_algorithm
    results = optimizer.optimize(kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1008, in optimize
    self.logger.log_optimizer_info(
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ResultLogger' object has no attribute 'log_optimizer_info'
[2025-03-12 00:12:26] Successfully completed jade optimization
[2025-03-12 00:12:31] ----------------------------------------
[2025-03-12 00:12:31] Running lshade algorithm
[2025-03-12 00:12:31] ----------------------------------------
[2025-03-12 00:12:31] Starting optimization with lshade algorithm...
/home/user/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

==================================================
Starting SPINN optimization for Klein-Gordon equation
==================================================

Configuration:
Algorithm: LSHADE
NC (collocation points): 1000
NI (initial points): 100
NB (boundary points): 100
NC_TEST (test points): 50
Random seed: 42
Total epochs: 10000
Number of trials: 150
Timeout: None

Results will be saved to: /home/user/SPINN_PyTorch/results/klein_gordon3d/spinn_clear/lshade_20250312_001233
Run parameters saved to run_params.json

============================================================
Initializing LSHADE optimizer
============================================================
Algorithm description:
L-SHADE (Linear Success-History based Adaptive DE) - Улучшенная версия SHADE алгоритма. Использует линейное уменьшение размера популяции и историю успешных решений. Особенно эффективен для задач большой размерности.
------------------------------------------------------------
Configuration:
  Number of trials: 150
  Timeout: None
  Algorithm parameters: {'population_size': 32}
------------------------------------------------------------

Starting optimization process...

============================================================
Starting optimization process
============================================================
Parameter bounds:
  n_layers            : (2, 5)
  layer_size          : (16, 128)
  lr_adamw            : (0.0001, 0.01)
  scheduler_factor    : (0.1, 0.5)
  scheduler_patience  : (5, 20)
  scheduler_min_lr    : (1e-06, 0.0001)
  scheduler_T_max     : (50, 200)
  scheduler_eta_min   : (1e-06, 0.0001)
------------------------------------------------------------

Initializing LSHADE optimizer...

Starting optimization...

------------------------------------------------------------
Trial 1/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 59
      Eta min: 8.68e-05

Training for 10000 epochs...

************************************************************
New best error found: 2.12e-02
************************************************************

Trial 1 completed:
  Current error: 2.12e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 2/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: relu
  Optimizers:
    AdamW lr: 3.04e-04
  Scheduler: none

Training for 10000 epochs...

Trial 2 completed:
  Current error: 1.00e+00
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 3/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: silu
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 94
      Eta min: 3.73e-05

Training for 10000 epochs...

Trial 3 completed:
  Current error: 2.18e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 4/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [104, 104, 104]
    Activation: silu
  Optimizers:
    AdamW lr: 2.08e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 141
      Eta min: 1.79e-05

Training for 10000 epochs...

Trial 4 completed:
  Current error: 3.21e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 5/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [122, 122]
    Activation: silu
  Optimizers:
    AdamW lr: 9.66e-03
  Scheduler: none

Training for 10000 epochs...

Trial 5 completed:
  Current error: 1.79e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 6/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: relu
  Optimizers:
    AdamW lr: 4.40e-04
  Scheduler: none

Training for 10000 epochs...

Trial 6 completed:
  Current error: 9.98e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 7/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [37, 37, 37, 37]
    Activation: relu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: none

Training for 10000 epochs...

Trial 7 completed:
  Current error: 7.99e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 8/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [38, 38]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 174
      Eta min: 3.63e-05

Training for 10000 epochs...

Trial 8 completed:
  Current error: 1.26e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 9/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [77, 77, 77]
    Activation: relu
  Optimizers:
    AdamW lr: 1.50e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 166
      Eta min: 2.07e-05

Training for 10000 epochs...

Trial 9 completed:
  Current error: 9.99e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 10/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [107, 107]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.10e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.39
      Patience: 17
      Min lr: 8.33e-06

Training for 10000 epochs...

Trial 10 completed:
  Current error: 9.48e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 11/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [86, 86, 86, 86, 86]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.38e-03
  Scheduler: none

Training for 10000 epochs...

Trial 11 completed:
  Current error: 7.49e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 12/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [69, 69, 69, 69, 69]
    Activation: relu
  Optimizers:
    AdamW lr: 1.28e-03
  Scheduler: none

Training for 10000 epochs...

Trial 12 completed:
  Current error: 1.00e+00
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 13/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [64, 64, 64, 64]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.52e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 5.13e-05

Training for 10000 epochs...

Trial 13 completed:
  Current error: 4.74e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 14/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 93
      Eta min: 1.70e-05

Training for 10000 epochs...

Trial 14 completed:
  Current error: 2.55e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 15/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 184
      Eta min: 5.44e-05

Training for 10000 epochs...

Trial 15 completed:
  Current error: 2.57e-02
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 16/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [116, 116, 116, 116]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.25e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.14
      Patience: 8
      Min lr: 4.33e-05

Training for 10000 epochs...

Trial 16 completed:
  Current error: 1.56e-01
  Best error so far: 2.12e-02

------------------------------------------------------------
Trial 17/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [73, 73]
    Activation: silu
  Optimizers:
    AdamW lr: 4.23e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 191
      Eta min: 3.30e-05

Training for 10000 epochs...

************************************************************
New best error found: 1.76e-02
************************************************************

Trial 17 completed:
  Current error: 1.76e-02
  Best error so far: 1.76e-02

------------------------------------------------------------
Trial 18/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 19
      Min lr: 2.59e-05

Training for 10000 epochs...

Trial 18 completed:
  Current error: 4.64e-02
  Best error so far: 1.76e-02

------------------------------------------------------------
Trial 19/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [20, 20, 20]
    Activation: silu
  Optimizers:
    AdamW lr: 6.13e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.30
      Patience: 6
      Min lr: 2.86e-05

Training for 10000 epochs...

Trial 19 completed:
  Current error: 6.74e-01
  Best error so far: 1.76e-02

------------------------------------------------------------
Trial 20/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: silu
  Optimizers:
    AdamW lr: 9.86e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.20
      Patience: 15
      Min lr: 7.64e-05

Training for 10000 epochs...

Trial 20 completed:
  Current error: 2.28e-01
  Best error so far: 1.76e-02

------------------------------------------------------------
Trial 21/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: silu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 98
      Eta min: 1.95e-05

Training for 10000 epochs...

************************************************************
New best error found: 1.15e-02
************************************************************

Trial 21 completed:
  Current error: 1.15e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 22/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [82, 82]
    Activation: silu
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 147
      Eta min: 1.83e-05

Training for 10000 epochs...

Trial 22 completed:
  Current error: 2.10e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 23/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: silu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: none

Training for 10000 epochs...

Trial 23 completed:
  Current error: 2.32e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 24/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [90, 90, 90]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.19e-03
  Scheduler: none

Training for 10000 epochs...

Trial 24 completed:
  Current error: 2.40e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 25/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [87, 87, 87, 87, 87]
    Activation: silu
  Optimizers:
    AdamW lr: 3.46e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 183
      Eta min: 7.82e-05

Training for 10000 epochs...

Trial 25 completed:
  Current error: 2.16e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 26/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [25, 25, 25, 25]
    Activation: relu
  Optimizers:
    AdamW lr: 1.70e-03
  Scheduler: none

Training for 10000 epochs...

Trial 26 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 27/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 157
      Eta min: 2.45e-05

Training for 10000 epochs...

Trial 27 completed:
  Current error: 2.94e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 28/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [100, 100, 100]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.44
      Patience: 15
      Min lr: 5.73e-05

Training for 10000 epochs...

Trial 28 completed:
  Current error: 1.07e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 29/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [43, 43, 43]
    Activation: relu
  Optimizers:
    AdamW lr: 9.73e-03
  Scheduler: none

Training for 10000 epochs...

Trial 29 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 30/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [71, 71, 71, 71]
    Activation: relu
  Optimizers:
    AdamW lr: 2.03e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.39
      Patience: 9
      Min lr: 3.41e-06

Training for 10000 epochs...

Trial 30 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 31/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [123, 123, 123, 123, 123]
    Activation: relu
  Optimizers:
    AdamW lr: 9.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 114
      Eta min: 9.67e-05

Training for 10000 epochs...

Trial 31 completed:
  Current error: 9.98e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 32/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [112, 112, 112, 112, 112]
    Activation: relu
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 75
      Eta min: 5.61e-05

Training for 10000 epochs...

Trial 32 completed:
  Current error: 8.81e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 33/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [128, 128, 128]
    Activation: relu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.34
      Patience: 5
      Min lr: 1.64e-05

Training for 10000 epochs...

Trial 33 completed:
  Current error: 1.02e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 34/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [95, 95]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.90e-03
  Scheduler: none

Training for 10000 epochs...

Trial 34 completed:
  Current error: 5.42e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 35/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [73, 73]
    Activation: silu
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 88
      Eta min: 3.73e-05

Training for 10000 epochs...

Trial 35 completed:
  Current error: 1.63e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 36/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [104, 104, 104]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.08e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.26
      Patience: 9
      Min lr: 3.99e-05

Training for 10000 epochs...

Trial 36 completed:
  Current error: 1.38e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 37/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [122, 122]
    Activation: relu
  Optimizers:
    AdamW lr: 9.66e-03
  Scheduler: none

Training for 10000 epochs...

Trial 37 completed:
  Current error: 9.98e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 38/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: silu
  Optimizers:
    AdamW lr: 5.24e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.46
      Patience: 9
      Min lr: 6.66e-05

Training for 10000 epochs...

Trial 38 completed:
  Current error: 1.30e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 39/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [22, 22, 22, 22]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: none

Training for 10000 epochs...

Trial 39 completed:
  Current error: 1.87e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 40/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [38, 38]
    Activation: relu
  Optimizers:
    AdamW lr: 5.44e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 148
      Eta min: 3.63e-05

Training for 10000 epochs...

Trial 40 completed:
  Current error: 9.99e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 41/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [91, 91, 91]
    Activation: silu
  Optimizers:
    AdamW lr: 2.10e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 7
      Min lr: 7.94e-05

Training for 10000 epochs...

Trial 41 completed:
  Current error: 6.74e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 42/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [72, 72]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.66e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 75
      Eta min: 1.25e-05

Training for 10000 epochs...

Trial 42 completed:
  Current error: 5.58e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 43/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [109, 109, 109, 109, 109]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.76e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 7
      Min lr: 3.32e-05

Training for 10000 epochs...

Trial 43 completed:
  Current error: 2.80e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 44/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [69, 69, 69, 69, 69]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.83e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.29
      Patience: 16
      Min lr: 5.66e-05

Training for 10000 epochs...

Trial 44 completed:
  Current error: 8.27e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 45/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [64, 64, 64, 64]
    Activation: silu
  Optimizers:
    AdamW lr: 3.52e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 5.13e-05

Training for 10000 epochs...

Trial 45 completed:
  Current error: 3.73e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 46/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 46 completed:
  Current error: 2.61e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 47/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [107, 107, 107, 107]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: none

Training for 10000 epochs...

Trial 47 completed:
  Current error: 3.27e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 48/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [118, 118, 118, 118]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.12e-03
  Scheduler: none

Training for 10000 epochs...

Trial 48 completed:
  Current error: 1.28e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 49/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [73, 73]
    Activation: relu
  Optimizers:
    AdamW lr: 5.43e-03
  Scheduler: none

Training for 10000 epochs...

Trial 49 completed:
  Current error: 1.03e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 50/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [95, 95, 95]
    Activation: silu
  Optimizers:
    AdamW lr: 7.55e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.35
      Patience: 13
      Min lr: 2.59e-05

Training for 10000 epochs...

Trial 50 completed:
  Current error: 2.91e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 51/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [53, 53, 53]
    Activation: silu
  Optimizers:
    AdamW lr: 6.13e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.30
      Patience: 5
      Min lr: 7.11e-05

Training for 10000 epochs...

Trial 51 completed:
  Current error: 1.04e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 52/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [59, 59]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 56
      Eta min: 7.31e-05

Training for 10000 epochs...

Trial 52 completed:
  Current error: 5.29e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 53/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.41
      Patience: 6
      Min lr: 6.27e-05

Training for 10000 epochs...

Trial 53 completed:
  Current error: 1.07e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 54/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [82, 82]
    Activation: silu
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: none

Training for 10000 epochs...

Trial 54 completed:
  Current error: 2.10e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 55/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [78, 78]
    Activation: relu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 189
      Eta min: 8.79e-05

Training for 10000 epochs...

Trial 55 completed:
  Current error: 1.05e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 56/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [81, 81, 81]
    Activation: relu
  Optimizers:
    AdamW lr: 6.30e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 13
      Min lr: 7.65e-05

Training for 10000 epochs...

Trial 56 completed:
  Current error: 1.02e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 57/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [87, 87, 87, 87, 87]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.84e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.24
      Patience: 16
      Min lr: 8.98e-05

Training for 10000 epochs...

Trial 57 completed:
  Current error: 5.58e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 58/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [69, 69, 69, 69]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.56e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.46
      Patience: 14
      Min lr: 3.55e-05

Training for 10000 epochs...

Trial 58 completed:
  Current error: 6.90e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 59/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [34, 34, 34]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.55e-03
  Scheduler: none

Training for 10000 epochs...

Trial 59 completed:
  Current error: 5.53e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 60/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [100, 100, 100, 100]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.53e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 64
      Eta min: 3.74e-05

Training for 10000 epochs...

Trial 60 completed:
  Current error: 3.88e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 61/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [76, 76, 76]
    Activation: silu
  Optimizers:
    AdamW lr: 9.73e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 169
      Eta min: 5.08e-05

Training for 10000 epochs...

Trial 61 completed:
  Current error: 2.08e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 62/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [71, 71, 71, 71]
    Activation: relu
  Optimizers:
    AdamW lr: 7.05e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.45
      Patience: 10
      Min lr: 3.41e-06

Training for 10000 epochs...

Trial 62 completed:
  Current error: 1.01e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 63/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [123, 123, 123, 123, 123]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 114
      Eta min: 9.67e-05

Training for 10000 epochs...

Trial 63 completed:
  Current error: 7.07e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 64/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [112, 112, 112, 112, 112]
    Activation: relu
  Optimizers:
    AdamW lr: 3.52e-03
  Scheduler: none

Training for 10000 epochs...

Trial 64 completed:
  Current error: 9.98e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 65/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [101, 101, 101]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 6
      Min lr: 7.74e-05

Training for 10000 epochs...

Trial 65 completed:
  Current error: 1.79e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 66/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [103, 103]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 88
      Eta min: 3.07e-05

Training for 10000 epochs...

Trial 66 completed:
  Current error: 3.51e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 67/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [66, 66]
    Activation: relu
  Optimizers:
    AdamW lr: 4.23e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.29
      Patience: 10
      Min lr: 2.05e-05

Training for 10000 epochs...

Trial 67 completed:
  Current error: 1.01e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 68/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [22, 22, 22, 22]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 120
      Eta min: 9.23e-05

Training for 10000 epochs...

Trial 68 completed:
  Current error: 7.89e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 69/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [76, 76, 76]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.73e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 169
      Eta min: 5.08e-05

Training for 10000 epochs...

Trial 69 completed:
  Current error: 1.52e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 70/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [91, 91]
    Activation: silu
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 13
      Min lr: 2.34e-05

Training for 10000 epochs...

Trial 70 completed:
  Current error: 2.04e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 71/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [128, 128]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: none

Training for 10000 epochs...

Trial 71 completed:
  Current error: 1.42e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 72/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [79, 79, 79]
    Activation: relu
  Optimizers:
    AdamW lr: 5.91e-03
  Scheduler: none

Training for 10000 epochs...

Trial 72 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 73/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.47e-03
  Scheduler: none

Training for 10000 epochs...

Trial 73 completed:
  Current error: 2.15e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 74/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [90, 90, 90, 90]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.67e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 51
      Eta min: 8.98e-05

Training for 10000 epochs...

Trial 74 completed:
  Current error: 2.16e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 75/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [85, 85, 85, 85, 85]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.12e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 93
      Eta min: 1.96e-05

Training for 10000 epochs...

Trial 75 completed:
  Current error: 8.79e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 76/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: relu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 143
      Eta min: 5.44e-05

Training for 10000 epochs...

Trial 76 completed:
  Current error: 1.00e+00
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 77/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.09e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 79
      Eta min: 2.34e-05

Training for 10000 epochs...

Trial 77 completed:
  Current error: 7.82e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 78/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [54, 54]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: none

Training for 10000 epochs...

Trial 78 completed:
  Current error: 8.74e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 79/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [111, 111, 111, 111]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.73e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 14
      Min lr: 5.60e-06

Training for 10000 epochs...

Trial 79 completed:
  Current error: 9.66e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 80/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [64, 64, 64, 64]
    Activation: silu
  Optimizers:
    AdamW lr: 3.52e-04
  Scheduler: none

Training for 10000 epochs...

Trial 80 completed:
  Current error: 3.73e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 81/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [100, 100, 100, 100]
    Activation: relu
  Optimizers:
    AdamW lr: 6.53e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 86
      Eta min: 1.22e-05

Training for 10000 epochs...

Trial 81 completed:
  Current error: 9.98e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 82/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [59, 59, 59]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 8.05e-05

Training for 10000 epochs...

Trial 82 completed:
  Current error: 2.66e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 83/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [95, 95]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.90e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 11
      Min lr: 6.57e-05

Training for 10000 epochs...

Trial 83 completed:
  Current error: 1.90e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 84/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [86, 86]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.66e-03
  Scheduler: none

Training for 10000 epochs...

Trial 84 completed:
  Current error: 1.69e-02
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 85/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [92, 92, 92]
    Activation: tanh
  Optimizers:
    AdamW lr: 2.10e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.23
      Patience: 5
      Min lr: 6.45e-05

Training for 10000 epochs...

Trial 85 completed:
  Current error: 1.17e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 86/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [123, 123, 123, 123]
    Activation: silu
  Optimizers:
    AdamW lr: 9.16e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 5
      Min lr: 7.29e-05

Training for 10000 epochs...

Trial 86 completed:
  Current error: 1.70e-01
  Best error so far: 1.15e-02

------------------------------------------------------------
Trial 87/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [102, 102, 102, 102, 102]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 130
      Eta min: 7.49e-05

Training for 10000 epochs...

************************************************************
New best error found: 1.14e-02
************************************************************

Trial 87 completed:
  Current error: 1.14e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 88/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [69, 69, 69, 69, 69]
    Activation: relu
  Optimizers:
    AdamW lr: 3.83e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.29
      Patience: 11
      Min lr: 5.66e-05

Training for 10000 epochs...

Trial 88 completed:
  Current error: 1.00e+00
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 89/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [53, 53, 53]
    Activation: relu
  Optimizers:
    AdamW lr: 6.13e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.17
      Patience: 11
      Min lr: 7.11e-05

Training for 10000 epochs...

Trial 89 completed:
  Current error: 1.01e+00
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 90/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [38, 38, 38, 38]
    Activation: relu
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.23
      Patience: 11
      Min lr: 3.60e-05

Training for 10000 epochs...

Trial 90 completed:
  Current error: 9.99e-01
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 91/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [118, 118, 118, 118]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.12e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.10
      Patience: 8
      Min lr: 4.23e-05

Training for 10000 epochs...

Trial 91 completed:
  Current error: 3.17e-01
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 92/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [79, 79, 79]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.24e-03
  Scheduler: none

Training for 10000 epochs...

Trial 92 completed:
  Current error: 2.40e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 93/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [116, 116, 116]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 10
      Min lr: 4.24e-05

Training for 10000 epochs...

Trial 93 completed:
  Current error: 7.49e-01
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 94/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [90, 90, 90, 90]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.56e-03
  Scheduler: none

Training for 10000 epochs...

Trial 94 completed:
  Current error: 2.98e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 95/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [110, 110, 110, 110, 110]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: none

Training for 10000 epochs...

Trial 95 completed:
  Current error: 7.76e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 96/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [71, 71, 71, 71, 71]
    Activation: silu
  Optimizers:
    AdamW lr: 1.00e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 106
      Eta min: 6.27e-05

Training for 10000 epochs...

Trial 96 completed:
  Current error: 4.32e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 97/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [102, 102, 102, 102, 102]
    Activation: silu
  Optimizers:
    AdamW lr: 5.22e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 130
      Eta min: 7.49e-05

Training for 10000 epochs...

Trial 97 completed:
  Current error: 1.36e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 98/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [119, 119, 119]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.28e-03
  Scheduler: none

Training for 10000 epochs...

Trial 98 completed:
  Current error: 3.27e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 99/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [106, 106, 106]
    Activation: tanh
  Optimizers:
    AdamW lr: 2.49e-03
  Scheduler: none

Training for 10000 epochs...

Trial 99 completed:
  Current error: 5.46e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 100/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [86, 86]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.52e-03
  Scheduler: none

Training for 10000 epochs...

Trial 100 completed:
  Current error: 7.91e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 101/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [73, 73, 73, 73]
    Activation: relu
  Optimizers:
    AdamW lr: 2.45e-03
  Scheduler: none

Training for 10000 epochs...

Trial 101 completed:
  Current error: 9.99e-01
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 102/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [22, 22, 22, 22]
    Activation: relu
  Optimizers:
    AdamW lr: 2.77e-03
  Scheduler: none

Training for 10000 epochs...

Trial 102 completed:
  Current error: 9.99e-01
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 103/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [76, 76, 76]
    Activation: relu
  Optimizers:
    AdamW lr: 7.02e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 169
      Eta min: 9.15e-05

Training for 10000 epochs...

Trial 103 completed:
  Current error: 9.99e-01
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 104/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 13
      Min lr: 1.29e-05

Training for 10000 epochs...

Trial 104 completed:
  Current error: 7.90e-01
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 105/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [122, 122, 122, 122, 122]
    Activation: silu
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: none

Training for 10000 epochs...

Trial 105 completed:
  Current error: 2.31e-02
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 106/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: relu
  Optimizers:
    AdamW lr: 7.73e-03
  Scheduler: none

Training for 10000 epochs...

Trial 106 completed:
  Current error: 9.99e-01
  Best error so far: 1.14e-02

------------------------------------------------------------
Trial 107/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [99, 99, 99, 99, 99]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.46e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.01e-02
************************************************************

Trial 107 completed:
  Current error: 1.01e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 108/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [90, 90, 90, 90]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.18
      Patience: 9
      Min lr: 1.00e-04

Training for 10000 epochs...

Trial 108 completed:
  Current error: 1.34e-01
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 109/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [95, 95, 95]
    Activation: silu
  Optimizers:
    AdamW lr: 6.91e-03
  Scheduler: none

Training for 10000 epochs...

Trial 109 completed:
  Current error: 1.75e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 110/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: silu
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.40
      Patience: 8
      Min lr: 8.62e-06

Training for 10000 epochs...

Trial 110 completed:
  Current error: 7.63e-01
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 111/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [128, 128, 128, 128]
    Activation: silu
  Optimizers:
    AdamW lr: 3.45e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 187
      Eta min: 2.92e-05

Training for 10000 epochs...

Trial 111 completed:
  Current error: 2.52e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 112/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [71, 71, 71]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 8.05e-05

Training for 10000 epochs...

Trial 112 completed:
  Current error: 2.79e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 113/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [79, 79, 79]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.55e-03
  Scheduler: none

Training for 10000 epochs...

Trial 113 completed:
  Current error: 2.29e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 114/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [45, 45]
    Activation: silu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: none

Training for 10000 epochs...

Trial 114 completed:
  Current error: 1.60e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 115/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [90, 90, 90, 90, 90]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.56e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 71
      Eta min: 4.90e-05

Training for 10000 epochs...

Trial 115 completed:
  Current error: 1.91e-01
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 116/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [104, 104, 104]
    Activation: silu
  Optimizers:
    AdamW lr: 2.08e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 141
      Eta min: 1.79e-05

Training for 10000 epochs...

Trial 116 completed:
  Current error: 3.21e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 117/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [91, 91, 91, 91]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.52e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.18
      Patience: 5
      Min lr: 6.07e-05

Training for 10000 epochs...

Trial 117 completed:
  Current error: 6.58e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 118/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [98, 98, 98, 98]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.21
      Patience: 15
      Min lr: 5.73e-05

Training for 10000 epochs...

Trial 118 completed:
  Current error: 1.48e-01
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 119/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [95, 95, 95, 95, 95]
    Activation: silu
  Optimizers:
    AdamW lr: 3.46e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 15
      Min lr: 1.00e-04

Training for 10000 epochs...

Trial 119 completed:
  Current error: 6.01e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 120/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [102, 102, 102, 102, 102]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.38e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.18
      Patience: 10
      Min lr: 3.32e-05

Training for 10000 epochs...

Trial 120 completed:
  Current error: 1.55e-01
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 121/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 117
      Eta min: 1.95e-05

Training for 10000 epochs...

Trial 121 completed:
  Current error: 2.21e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 122/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [56, 56]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.35e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.24
      Patience: 18
      Min lr: 7.43e-05

Training for 10000 epochs...

Trial 122 completed:
  Current error: 1.69e-01
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 123/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [73, 73, 73, 73]
    Activation: tanh
  Optimizers:
    AdamW lr: 2.31e-03
  Scheduler: none

Training for 10000 epochs...

Trial 123 completed:
  Current error: 4.11e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 124/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [96, 96, 96]
    Activation: silu
  Optimizers:
    AdamW lr: 5.66e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.24
      Patience: 17
      Min lr: 7.45e-05

Training for 10000 epochs...

Trial 124 completed:
  Current error: 1.26e-01
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 125/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [128, 128, 128, 128]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.15e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 117
      Eta min: 9.76e-05

Training for 10000 epochs...

Trial 125 completed:
  Current error: 2.78e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 126/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [73, 73]
    Activation: silu
  Optimizers:
    AdamW lr: 4.49e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 191
      Eta min: 3.30e-05

Training for 10000 epochs...

Trial 126 completed:
  Current error: 1.62e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 127/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [92, 92, 92, 92]
    Activation: silu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.29
      Patience: 19
      Min lr: 8.96e-05

Training for 10000 epochs...

Trial 127 completed:
  Current error: 2.14e-01
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 128/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [86, 86, 86]
    Activation: relu
  Optimizers:
    AdamW lr: 6.79e-03
  Scheduler: none

Training for 10000 epochs...

Trial 128 completed:
  Current error: 9.99e-01
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 129/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [86, 86, 86]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.30e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 147
      Eta min: 1.83e-05

Training for 10000 epochs...

Trial 129 completed:
  Current error: 2.18e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 130/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [116, 116, 116]
    Activation: silu
  Optimizers:
    AdamW lr: 3.25e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 89
      Eta min: 6.43e-05

Training for 10000 epochs...

Trial 130 completed:
  Current error: 2.06e-02
  Best error so far: 1.01e-02

------------------------------------------------------------
Trial 131/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.47e-03
  Scheduler: none

Training for 10000 epochs...

Trial 131 completed:
  Current error: 2.15e-02
  Best error so far: 1.01e-02

============================================================
Optimization completed successfully!
============================================================
Total time: 44079.11 seconds
Best objective value: 1.01e-02

Best parameters found:
------------------------------------------------------------
Architecture:
  Number of layers: 5
  Layer sizes: [99, 99, 99, 99, 99]
  Activation: silu

Optimizers:
  AdamW learning rate: 3.46e-03

Scheduler type: reduce_on_plateau
  Parameters:
    Factor: 0.24
    Patience: 15
    Min lr: 8.98e-05
------------------------------------------------------------

Saving results to logger...
Traceback (most recent call last):
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1305, in <module>
    main(args.nc, args.ni, args.nb, args.nc_test, args.seed, args.epochs)
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1279, in main
    main_with_algorithm(args.algorithm, args.n_trials, args.timeout, algorithm_params=algorithm_params, **params)
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1134, in main_with_algorithm
    results = optimizer.optimize(kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1008, in optimize
    self.logger.log_optimizer_info(
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ResultLogger' object has no attribute 'log_optimizer_info'
[2025-03-12 12:27:13] Successfully completed lshade optimization
[2025-03-12 12:27:18] ----------------------------------------
[2025-03-12 12:27:18] All algorithms completed successfully
[2025-03-12 12:27:18] ----------------------------------------
