[2025-03-07 18:04:17] Starting optimization process for all algorithms
[2025-03-07 18:04:17] ----------------------------------------
[2025-03-07 18:04:17] Running jade algorithm
[2025-03-07 18:04:17] ----------------------------------------
[2025-03-07 18:04:17] Starting optimization with jade algorithm...
/home/user/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

==================================================
Starting SPINN optimization for Klein-Gordon equation
==================================================

Configuration:
Algorithm: JADE
NC (collocation points): 1000
NI (initial points): 100
NB (boundary points): 100
NC_TEST (test points): 50
Random seed: 42
Total epochs: 10000
Number of trials: 150
Timeout: None

Results will be saved to: /home/user/SPINN_PyTorch/results/klein_gordon3d/spinn_clear/jade_20250307_180419
Run parameters saved to run_params.json

============================================================
Initializing JADE optimizer
============================================================
Algorithm description:
JADE (Adaptive Differential Evolution) - Адаптивный алгоритм дифференциальной эволюции. Автоматически адаптирует параметры мутации и скрещивания. Эффективен для непрерывной оптимизации и хорошо масштабируется.
------------------------------------------------------------
Configuration:
  Number of trials: 150
  Timeout: None
  Algorithm parameters: {'population_size': 100, 'c': 0.1, 'p': 0.05}
------------------------------------------------------------

Starting optimization process...

============================================================
Starting optimization process
============================================================
Parameter bounds:
  n_layers            : (2, 5)
  layer_size          : (16, 128)
  lr_adamw            : (0.0001, 0.01)
  scheduler_factor    : (0.1, 0.5)
  scheduler_patience  : (5, 20)
  scheduler_min_lr    : (1e-06, 0.0001)
  scheduler_T_max     : (50, 200)
  scheduler_eta_min   : (1e-06, 0.0001)
------------------------------------------------------------

Initializing JADE optimizer...

Starting optimization...

------------------------------------------------------------
Trial 1/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: silu
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 2.10e-02
************************************************************

Trial 1 completed:
  Current error: 2.10e-02
  Best error so far: 2.10e-02

------------------------------------------------------------
Trial 2/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: relu
  Optimizers:
    AdamW lr: 3.04e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 17
      Min lr: 2.20e-05

Training for 10000 epochs...

Trial 2 completed:
  Current error: 1.00e+00
  Best error so far: 2.10e-02

------------------------------------------------------------
Trial 3/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: silu
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.88e-02
************************************************************

Trial 3 completed:
  Current error: 1.88e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 4/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [104, 104, 104]
    Activation: relu
  Optimizers:
    AdamW lr: 2.08e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 14
      Min lr: 5.60e-06

Training for 10000 epochs...

Trial 4 completed:
  Current error: 9.99e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 5/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [122, 122]
    Activation: relu
  Optimizers:
    AdamW lr: 9.66e-03
  Scheduler: none

Training for 10000 epochs...

Trial 5 completed:
  Current error: 9.98e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 6/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.40e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 5.25e-05

Training for 10000 epochs...

Trial 6 completed:
  Current error: 3.84e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 7/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [37, 37, 37, 37]
    Activation: silu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.41
      Patience: 19
      Min lr: 8.96e-05

Training for 10000 epochs...

Trial 7 completed:
  Current error: 2.49e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 8/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [38, 38]
    Activation: relu
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: none

Training for 10000 epochs...

Trial 8 completed:
  Current error: 9.99e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 9/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [77, 77, 77]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.50e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.42
      Patience: 6
      Min lr: 9.87e-05

Training for 10000 epochs...

Trial 9 completed:
  Current error: 1.04e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 10/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [107, 107]
    Activation: relu
  Optimizers:
    AdamW lr: 7.10e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.39
      Patience: 17
      Min lr: 8.33e-06

Training for 10000 epochs...

Trial 10 completed:
  Current error: 1.00e+00
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 11/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [86, 86, 86, 86, 86]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 159
      Eta min: 6.41e-05

Training for 10000 epochs...

Trial 11 completed:
  Current error: 7.49e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 12/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [69, 69, 69, 69, 69]
    Activation: relu
  Optimizers:
    AdamW lr: 1.28e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 166
      Eta min: 4.99e-05

Training for 10000 epochs...

Trial 12 completed:
  Current error: 1.00e+00
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 13/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [64, 64, 64, 64]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.52e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.14
      Patience: 5
      Min lr: 6.40e-05

Training for 10000 epochs...

Trial 13 completed:
  Current error: 9.12e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 14/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 14 completed:
  Current error: 4.62e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 15/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: relu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: none

Training for 10000 epochs...

Trial 15 completed:
  Current error: 1.00e+00
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 16/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [116, 116, 116, 116]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.25e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.72e-02
************************************************************

Trial 16 completed:
  Current error: 1.72e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 17/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [73, 73]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.23e-03
  Scheduler: none

Training for 10000 epochs...

Trial 17 completed:
  Current error: 5.15e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 18/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 19
      Min lr: 2.59e-05

Training for 10000 epochs...

Trial 18 completed:
  Current error: 1.27e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 19/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [20, 20, 20]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.13e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 186
      Eta min: 2.47e-05

Training for 10000 epochs...

Trial 19 completed:
  Current error: 6.75e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 20/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: silu
  Optimizers:
    AdamW lr: 9.86e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.20
      Patience: 15
      Min lr: 7.64e-05

Training for 10000 epochs...

Trial 20 completed:
  Current error: 2.32e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 21/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 98
      Eta min: 1.95e-05

Training for 10000 epochs...

Trial 21 completed:
  Current error: 9.18e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 22/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [82, 82]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: none

Training for 10000 epochs...

Trial 22 completed:
  Current error: 5.79e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 23/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.16
      Patience: 10
      Min lr: 1.22e-05

Training for 10000 epochs...

Trial 23 completed:
  Current error: 5.16e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 24/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [90, 90, 90]
    Activation: relu
  Optimizers:
    AdamW lr: 8.19e-03
  Scheduler: none

Training for 10000 epochs...

Trial 24 completed:
  Current error: 1.01e+00
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 25/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [87, 87, 87, 87, 87]
    Activation: relu
  Optimizers:
    AdamW lr: 3.46e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 183
      Eta min: 7.82e-05

Training for 10000 epochs...

Trial 25 completed:
  Current error: 1.00e+00
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 26/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [25, 25, 25, 25]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.70e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 65
      Eta min: 6.67e-05

Training for 10000 epochs...

Trial 26 completed:
  Current error: 1.08e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 27/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 157
      Eta min: 2.45e-05

Training for 10000 epochs...

Trial 27 completed:
  Current error: 2.93e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 28/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [100, 100, 100]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.44
      Patience: 15
      Min lr: 5.73e-05

Training for 10000 epochs...

Trial 28 completed:
  Current error: 1.09e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 29/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [43, 43, 43]
    Activation: silu
  Optimizers:
    AdamW lr: 9.73e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.67e-02
************************************************************

Trial 29 completed:
  Current error: 1.67e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 30/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [71, 71, 71, 71]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.03e-03
  Scheduler: none

Training for 10000 epochs...

Trial 30 completed:
  Current error: 4.32e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 31/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [123, 123, 123, 123, 123]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 114
      Eta min: 9.67e-05

Training for 10000 epochs...

Trial 31 completed:
  Current error: 8.30e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 32/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [112, 112, 112, 112, 112]
    Activation: silu
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 18
      Min lr: 3.24e-05

Training for 10000 epochs...

Trial 32 completed:
  Current error: 7.73e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 33/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [94, 94, 94, 94, 94]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.74e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 71
      Eta min: 5.23e-05

Training for 10000 epochs...

Trial 33 completed:
  Current error: 1.98e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 34/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [99, 99, 99, 99, 99]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.00e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 10
      Min lr: 3.01e-05

Training for 10000 epochs...

Trial 34 completed:
  Current error: 7.46e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 35/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [118, 118, 118, 118, 118]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.16e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.30
      Patience: 17
      Min lr: 6.53e-05

Training for 10000 epochs...

Trial 35 completed:
  Current error: 7.07e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 36/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [54, 54, 54, 54, 54]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.82e-03
  Scheduler: none

Training for 10000 epochs...

Trial 36 completed:
  Current error: 2.43e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 37/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [82, 82, 82]
    Activation: relu
  Optimizers:
    AdamW lr: 4.02e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 69
      Eta min: 5.27e-05

Training for 10000 epochs...

Trial 37 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 38/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [40, 40, 40, 40]
    Activation: silu
  Optimizers:
    AdamW lr: 6.27e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 131
      Eta min: 6.41e-05

Training for 10000 epochs...

Trial 38 completed:
  Current error: 3.36e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 39/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [125, 125, 125, 125]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.21e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.23
      Patience: 17
      Min lr: 2.78e-05

Training for 10000 epochs...

Trial 39 completed:
  Current error: 2.33e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 40/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [124, 124]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.38e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 11
      Min lr: 1.82e-05

Training for 10000 epochs...

Trial 40 completed:
  Current error: 3.98e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 41/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [96, 96, 96, 96]
    Activation: silu
  Optimizers:
    AdamW lr: 6.64e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.21
      Patience: 19
      Min lr: 7.41e-05

Training for 10000 epochs...

Trial 41 completed:
  Current error: 9.92e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 42/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [44, 44, 44]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.62e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 57
      Eta min: 5.03e-06

Training for 10000 epochs...

Trial 42 completed:
  Current error: 4.97e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 43/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [95, 95, 95, 95, 95]
    Activation: relu
  Optimizers:
    AdamW lr: 4.79e-03
  Scheduler: none

Training for 10000 epochs...

Trial 43 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 44/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [85, 85, 85]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.39e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.12
      Patience: 11
      Min lr: 6.30e-05

Training for 10000 epochs...

Trial 44 completed:
  Current error: 1.66e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 45/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [34, 34, 34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.99e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.36
      Patience: 5
      Min lr: 5.90e-05

Training for 10000 epochs...

Trial 45 completed:
  Current error: 4.35e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 46/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [88, 88, 88]
    Activation: silu
  Optimizers:
    AdamW lr: 4.64e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 19
      Min lr: 3.92e-05

Training for 10000 epochs...

Trial 46 completed:
  Current error: 6.22e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 47/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [24, 24, 24]
    Activation: relu
  Optimizers:
    AdamW lr: 1.10e-03
  Scheduler: none

Training for 10000 epochs...

Trial 47 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 48/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [19, 19, 19, 19, 19]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 48 completed:
  Current error: 5.73e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 49/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [106, 106, 106, 106]
    Activation: silu
  Optimizers:
    AdamW lr: 2.89e-03
  Scheduler: none

Training for 10000 epochs...

Trial 49 completed:
  Current error: 3.37e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 50/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [103, 103, 103]
    Activation: relu
  Optimizers:
    AdamW lr: 3.47e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 163
      Eta min: 7.57e-05

Training for 10000 epochs...

Trial 50 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 51/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [117, 117]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.10e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.43
      Patience: 10
      Min lr: 8.97e-05

Training for 10000 epochs...

Trial 51 completed:
  Current error: 2.07e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 52/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [26, 26, 26, 26, 26]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.26e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.48
      Patience: 19
      Min lr: 5.78e-05

Training for 10000 epochs...

Trial 52 completed:
  Current error: 4.27e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 53/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [53, 53, 53]
    Activation: relu
  Optimizers:
    AdamW lr: 6.76e-03
  Scheduler: none

Training for 10000 epochs...

Trial 53 completed:
  Current error: 1.01e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 54/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [78, 78]
    Activation: relu
  Optimizers:
    AdamW lr: 4.47e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.46
      Patience: 10
      Min lr: 1.26e-05

Training for 10000 epochs...

Trial 54 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 55/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [27, 27, 27, 27]
    Activation: relu
  Optimizers:
    AdamW lr: 9.33e-04
  Scheduler: none

Training for 10000 epochs...

Trial 55 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 56/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [127, 127]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 198
      Eta min: 7.56e-05

Training for 10000 epochs...

Trial 56 completed:
  Current error: 2.42e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 57/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [25, 25, 25]
    Activation: relu
  Optimizers:
    AdamW lr: 7.79e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 67
      Eta min: 4.98e-05

Training for 10000 epochs...

Trial 57 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 58/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [68, 68]
    Activation: silu
  Optimizers:
    AdamW lr: 6.57e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.15
      Patience: 7
      Min lr: 6.53e-05

Training for 10000 epochs...

Trial 58 completed:
  Current error: 6.23e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 59/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [58, 58, 58, 58, 58]
    Activation: relu
  Optimizers:
    AdamW lr: 2.93e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 52
      Eta min: 9.70e-05

Training for 10000 epochs...

Trial 59 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 60/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [116, 116]
    Activation: relu
  Optimizers:
    AdamW lr: 5.32e-03
  Scheduler: none

Training for 10000 epochs...

Trial 60 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 61/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [94, 94, 94, 94]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.60e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.35
      Patience: 14
      Min lr: 9.02e-05

Training for 10000 epochs...

Trial 61 completed:
  Current error: 1.45e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 62/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [116, 116, 116, 116, 116]
    Activation: silu
  Optimizers:
    AdamW lr: 4.61e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.35
      Patience: 9
      Min lr: 1.96e-05

Training for 10000 epochs...

Trial 62 completed:
  Current error: 1.25e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 63/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [25, 25, 25, 25]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.75e-03
  Scheduler: none

Training for 10000 epochs...

Trial 63 completed:
  Current error: 3.66e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 64/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [34, 34, 34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.12e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 142
      Eta min: 4.24e-05

Training for 10000 epochs...

Trial 64 completed:
  Current error: 2.00e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 65/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [113, 113, 113, 113, 113]
    Activation: relu
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: none

Training for 10000 epochs...

Trial 65 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 66/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: relu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 112
      Eta min: 2.81e-05

Training for 10000 epochs...

Trial 66 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 67/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [113, 113]
    Activation: silu
  Optimizers:
    AdamW lr: 8.15e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.50
      Patience: 20
      Min lr: 5.60e-05

Training for 10000 epochs...

Trial 67 completed:
  Current error: 1.96e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 68/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.56e-03
  Scheduler: none

Training for 10000 epochs...

Trial 68 completed:
  Current error: 2.17e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 69/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [56, 56, 56, 56]
    Activation: silu
  Optimizers:
    AdamW lr: 1.22e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 128
      Eta min: 8.54e-05

Training for 10000 epochs...

Trial 69 completed:
  Current error: 2.98e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 70/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [79, 79, 79, 79]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.78e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.26
      Patience: 7
      Min lr: 3.85e-06

Training for 10000 epochs...

Trial 70 completed:
  Current error: 9.97e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 71/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [40, 40, 40, 40]
    Activation: silu
  Optimizers:
    AdamW lr: 1.45e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 10
      Min lr: 5.94e-05

Training for 10000 epochs...

Trial 71 completed:
  Current error: 1.40e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 72/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [55, 55, 55, 55, 55]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.19e-03
  Scheduler: none

Training for 10000 epochs...

Trial 72 completed:
  Current error: 7.88e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 73/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [120, 120]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.97e-03
  Scheduler: none

Training for 10000 epochs...

Trial 73 completed:
  Current error: 5.71e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 74/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [43, 43, 43, 43]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.51e-04
  Scheduler: none

Training for 10000 epochs...

Trial 74 completed:
  Current error: 6.40e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 75/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [55, 55, 55]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.98e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 79
      Eta min: 5.05e-06

Training for 10000 epochs...

Trial 75 completed:
  Current error: 4.73e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 76/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [47, 47, 47]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.85e-03
  Scheduler: none

Training for 10000 epochs...

Trial 76 completed:
  Current error: 5.94e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 77/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [93, 93, 93, 93]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.89e-04
  Scheduler: none

Training for 10000 epochs...

Trial 77 completed:
  Current error: 5.67e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 78/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [47, 47]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.08e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.40
      Patience: 8
      Min lr: 2.17e-05

Training for 10000 epochs...

Trial 78 completed:
  Current error: 4.02e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 79/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [57, 57, 57, 57]
    Activation: silu
  Optimizers:
    AdamW lr: 4.68e-03
  Scheduler: none

Training for 10000 epochs...

Trial 79 completed:
  Current error: 2.42e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 80/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [76, 76, 76, 76]
    Activation: silu
  Optimizers:
    AdamW lr: 1.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 90
      Eta min: 3.84e-05

Training for 10000 epochs...

Trial 80 completed:
  Current error: 5.10e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 81/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [52, 52]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.19e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 139
      Eta min: 6.82e-05

Training for 10000 epochs...

Trial 81 completed:
  Current error: 2.83e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 82/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [72, 72, 72, 72]
    Activation: silu
  Optimizers:
    AdamW lr: 9.61e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 115
      Eta min: 1.36e-05

Training for 10000 epochs...

Trial 82 completed:
  Current error: 4.68e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 83/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [57, 57, 57]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.49e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.33
      Patience: 10
      Min lr: 9.87e-05

Training for 10000 epochs...

Trial 83 completed:
  Current error: 2.28e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 84/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [33, 33]
    Activation: relu
  Optimizers:
    AdamW lr: 2.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.16
      Patience: 8
      Min lr: 2.92e-05

Training for 10000 epochs...

Trial 84 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 85/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [75, 75]
    Activation: relu
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 85 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 86/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [45, 45, 45, 45]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.79e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 136
      Eta min: 2.87e-05

Training for 10000 epochs...

Trial 86 completed:
  Current error: 5.54e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 87/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [37, 37, 37, 37]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.30e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 67
      Eta min: 6.15e-05

Training for 10000 epochs...

Trial 87 completed:
  Current error: 1.91e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 88/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [81, 81, 81]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.63e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.29
      Patience: 13
      Min lr: 6.13e-06

Training for 10000 epochs...

Trial 88 completed:
  Current error: 7.11e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 89/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [127, 127]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.29e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.42
      Patience: 9
      Min lr: 6.85e-05

Training for 10000 epochs...

Trial 89 completed:
  Current error: 2.20e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 90/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [62, 62, 62]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.55e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.47
      Patience: 17
      Min lr: 9.65e-05

Training for 10000 epochs...

Trial 90 completed:
  Current error: 1.11e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 91/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [36, 36, 36, 36, 36]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.58e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.40
      Patience: 14
      Min lr: 8.43e-05

Training for 10000 epochs...

Trial 91 completed:
  Current error: 1.25e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 92/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [34, 34, 34]
    Activation: silu
  Optimizers:
    AdamW lr: 1.73e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 104
      Eta min: 8.78e-05

Training for 10000 epochs...

Trial 92 completed:
  Current error: 1.14e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 93/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [107, 107, 107]
    Activation: relu
  Optimizers:
    AdamW lr: 4.45e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 12
      Min lr: 3.08e-05

Training for 10000 epochs...

Trial 93 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 94/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [117, 117, 117]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.90e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 19
      Min lr: 6.28e-05

Training for 10000 epochs...

Trial 94 completed:
  Current error: 2.02e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 95/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [54, 54, 54, 54]
    Activation: silu
  Optimizers:
    AdamW lr: 1.48e-03
  Scheduler: none

Training for 10000 epochs...

Trial 95 completed:
  Current error: 2.61e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 96/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [51, 51]
    Activation: relu
  Optimizers:
    AdamW lr: 2.56e-03
  Scheduler: none

Training for 10000 epochs...

Trial 96 completed:
  Current error: 1.01e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 97/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [108, 108, 108]
    Activation: relu
  Optimizers:
    AdamW lr: 1.20e-03
  Scheduler: none

Training for 10000 epochs...

Trial 97 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 98/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [97, 97, 97]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.23e-03
  Scheduler: none

Training for 10000 epochs...

Trial 98 completed:
  Current error: 2.88e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 99/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [118, 118, 118]
    Activation: silu
  Optimizers:
    AdamW lr: 5.88e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.26
      Patience: 12
      Min lr: 9.48e-05

Training for 10000 epochs...

Trial 99 completed:
  Current error: 1.28e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 100/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [84, 84, 84, 84]
    Activation: tanh
  Optimizers:
    AdamW lr: 2.79e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 154
      Eta min: 9.23e-05

Training for 10000 epochs...

Trial 100 completed:
  Current error: 8.57e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 101/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.34
      Patience: 7
      Min lr: 2.95e-05

Training for 10000 epochs...

Trial 101 completed:
  Current error: 2.83e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 102/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: silu
  Optimizers:
    AdamW lr: 3.04e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 17
      Min lr: 2.20e-05

Training for 10000 epochs...

Trial 102 completed:
  Current error: 1.20e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 103/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: relu
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: none

Training for 10000 epochs...

Trial 103 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 104/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [72, 72, 72]
    Activation: silu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 14
      Min lr: 2.49e-05

Training for 10000 epochs...

Trial 104 completed:
  Current error: 3.81e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 105/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [60, 60]
    Activation: silu
  Optimizers:
    AdamW lr: 7.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 10
      Min lr: 4.97e-05

Training for 10000 epochs...

Trial 105 completed:
  Current error: 2.41e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 106/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [48, 48]
    Activation: relu
  Optimizers:
    AdamW lr: 4.40e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 20
      Min lr: 6.66e-05

Training for 10000 epochs...

Trial 106 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 107/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [69, 69, 69, 69]
    Activation: silu
  Optimizers:
    AdamW lr: 9.03e-03
  Scheduler: none

Training for 10000 epochs...

Trial 107 completed:
  Current error: 2.19e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 108/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [38, 38]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: none

Training for 10000 epochs...

Trial 108 completed:
  Current error: 3.47e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 109/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [58, 58, 58]
    Activation: silu
  Optimizers:
    AdamW lr: 1.50e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 166
      Eta min: 2.07e-05

Training for 10000 epochs...

Trial 109 completed:
  Current error: 4.56e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 110/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [107, 107]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: cosine_annealing
    Parameters:
      T_max: 179
      Eta min: 4.66e-05

Training for 10000 epochs...

Trial 110 completed:
  Current error: 1.84e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 111/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [86, 86, 86]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 161
      Eta min: 6.41e-05

Training for 10000 epochs...

Trial 111 completed:
  Current error: 2.50e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 112/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [42, 42, 42]
    Activation: relu
  Optimizers:
    AdamW lr: 1.28e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.33
      Patience: 15
      Min lr: 5.66e-05

Training for 10000 epochs...

Trial 112 completed:
  Current error: 9.98e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 113/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [64, 64, 64, 64]
    Activation: relu
  Optimizers:
    AdamW lr: 6.80e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.40
      Patience: 5
      Min lr: 6.40e-05

Training for 10000 epochs...

Trial 113 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 114/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: relu
  Optimizers:
    AdamW lr: 6.61e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 112
      Eta min: 5.17e-05

Training for 10000 epochs...

Trial 114 completed:
  Current error: 9.98e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 115/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.90e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 185
      Eta min: 5.76e-05

Training for 10000 epochs...

Trial 115 completed:
  Current error: 9.94e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 116/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [116, 116, 116, 116]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.75e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.10
      Patience: 8
      Min lr: 5.04e-05

Training for 10000 epochs...

Trial 116 completed:
  Current error: 4.07e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 117/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [46, 46, 46, 46]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.80e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.19
      Patience: 12
      Min lr: 3.44e-05

Training for 10000 epochs...

Trial 117 completed:
  Current error: 2.02e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 118/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: relu
  Optimizers:
    AdamW lr: 9.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 125
      Eta min: 3.19e-05

Training for 10000 epochs...

Trial 118 completed:
  Current error: 1.02e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 119/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [54, 54, 54]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.13e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.30
      Patience: 16
      Min lr: 4.22e-05

Training for 10000 epochs...

Trial 119 completed:
  Current error: 1.42e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 120/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [58, 58, 58]
    Activation: silu
  Optimizers:
    AdamW lr: 9.86e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 92
      Eta min: 4.36e-05

Training for 10000 epochs...

************************************************************
New best error found: 1.49e-02
************************************************************

Trial 120 completed:
  Current error: 1.49e-02
  Best error so far: 1.49e-02

------------------------------------------------------------
Trial 121/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: silu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 6
      Min lr: 7.07e-05

Training for 10000 epochs...

Trial 121 completed:
  Current error: 1.12e-01
  Best error so far: 1.49e-02

------------------------------------------------------------
Trial 122/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [79, 79, 79]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 13
      Min lr: 3.14e-05

Training for 10000 epochs...

Trial 122 completed:
  Current error: 1.71e-01
  Best error so far: 1.49e-02

------------------------------------------------------------
Trial 123/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.16
      Patience: 8
      Min lr: 1.22e-05

Training for 10000 epochs...

Trial 123 completed:
  Current error: 9.99e-01
  Best error so far: 1.49e-02

------------------------------------------------------------
Trial 124/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [90, 90, 90]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.40e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 13
      Min lr: 2.49e-05

Training for 10000 epochs...

Trial 124 completed:
  Current error: 4.56e-01
  Best error so far: 1.49e-02

------------------------------------------------------------
Trial 125/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [107, 107, 107]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.46e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.24
      Patience: 16
      Min lr: 8.98e-05

Training for 10000 epochs...

Trial 125 completed:
  Current error: 1.03e-01
  Best error so far: 1.49e-02

------------------------------------------------------------
Trial 126/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [25, 25, 25, 25]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.58e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 65
      Eta min: 6.67e-05

Training for 10000 epochs...

Trial 126 completed:
  Current error: 5.12e-02
  Best error so far: 1.49e-02

------------------------------------------------------------
Trial 127/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 15
      Min lr: 2.23e-05

Training for 10000 epochs...

Trial 127 completed:
  Current error: 1.00e+00
  Best error so far: 1.49e-02

------------------------------------------------------------
Trial 128/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [100, 100]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.66e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.44
      Patience: 15
      Min lr: 5.73e-05

Training for 10000 epochs...

Trial 128 completed:
  Current error: 1.68e-01
  Best error so far: 1.49e-02

------------------------------------------------------------
Trial 129/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [43, 43, 43]
    Activation: silu
  Optimizers:
    AdamW lr: 9.73e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.48e-02
************************************************************

Trial 129 completed:
  Current error: 1.48e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 130/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [43, 43, 43, 43]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.94e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.39
      Patience: 17
      Min lr: 6.99e-05

Training for 10000 epochs...

Trial 130 completed:
  Current error: 1.63e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 131/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [123, 123, 123]
    Activation: silu
  Optimizers:
    AdamW lr: 9.57e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 114
      Eta min: 4.20e-05

Training for 10000 epochs...

Trial 131 completed:
  Current error: 4.63e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 132/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [70, 70, 70, 70, 70]
    Activation: relu
  Optimizers:
    AdamW lr: 8.81e-03
  Scheduler: none

Training for 10000 epochs...

Trial 132 completed:
  Current error: 1.01e+00
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 133/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [94, 94, 94, 94]
    Activation: silu
  Optimizers:
    AdamW lr: 5.74e-03
  Scheduler: none

Training for 10000 epochs...

Trial 133 completed:
  Current error: 2.29e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 134/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [47, 47, 47, 47]
    Activation: relu
  Optimizers:
    AdamW lr: 7.00e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 10
      Min lr: 3.01e-05

Training for 10000 epochs...

Trial 134 completed:
  Current error: 9.98e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 135/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [115, 115, 115, 115, 115]
    Activation: relu
  Optimizers:
    AdamW lr: 5.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 126
      Eta min: 7.98e-05

Training for 10000 epochs...

Trial 135 completed:
  Current error: 9.83e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 136/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [51, 51, 51]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.95e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 185
      Eta min: 5.47e-05

Training for 10000 epochs...

Trial 136 completed:
  Current error: 5.20e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 137/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [49, 49, 49]
    Activation: silu
  Optimizers:
    AdamW lr: 4.02e-04
  Scheduler: none

Training for 10000 epochs...

Trial 137 completed:
  Current error: 6.78e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 138/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [40, 40, 40, 40]
    Activation: relu
  Optimizers:
    AdamW lr: 6.27e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 74
      Eta min: 6.62e-05

Training for 10000 epochs...

Trial 138 completed:
  Current error: 1.00e+00
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 139/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [99, 99, 99, 99]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.21e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.23
      Patience: 20
      Min lr: 2.78e-05

Training for 10000 epochs...

Trial 139 completed:
  Current error: 3.85e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 140/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [98, 98]
    Activation: relu
  Optimizers:
    AdamW lr: 8.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 73
      Eta min: 2.58e-05

Training for 10000 epochs...

Trial 140 completed:
  Current error: 9.56e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 141/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [46, 46, 46, 46]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.32e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.21
      Patience: 17
      Min lr: 8.17e-05

Training for 10000 epochs...

Trial 141 completed:
  Current error: 1.17e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 142/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [56, 56, 56]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.86e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.40
      Patience: 5
      Min lr: 2.56e-05

Training for 10000 epochs...

Trial 142 completed:
  Current error: 2.45e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 143/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [95, 95, 95, 95, 95]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.79e-03
  Scheduler: none

Training for 10000 epochs...

Trial 143 completed:
  Current error: 1.63e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 144/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [27, 27, 27]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: cosine_annealing
    Parameters:
      T_max: 125
      Eta min: 8.58e-05

Training for 10000 epochs...

Trial 144 completed:
  Current error: 4.19e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 145/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [40, 40, 40, 40]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.49e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.36
      Patience: 16
      Min lr: 5.90e-05

Training for 10000 epochs...

Trial 145 completed:
  Current error: 6.12e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 146/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [88, 88, 88, 88]
    Activation: relu
  Optimizers:
    AdamW lr: 6.58e-03
  Scheduler: none

Training for 10000 epochs...

Trial 146 completed:
  Current error: 1.01e+00
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 147/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [24, 24, 24]
    Activation: silu
  Optimizers:
    AdamW lr: 8.20e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 16
      Min lr: 8.63e-05

Training for 10000 epochs...

Trial 147 completed:
  Current error: 2.78e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 148/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [19, 19, 19]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.56e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 128
      Eta min: 5.50e-05

Training for 10000 epochs...

Trial 148 completed:
  Current error: 9.29e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 149/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [106, 106, 106]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.89e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 200
      Eta min: 4.65e-05

Training for 10000 epochs...

Trial 149 completed:
  Current error: 2.45e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 150/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: silu
  Optimizers:
    AdamW lr: 3.47e-03
  Scheduler: none

Training for 10000 epochs...

Trial 150 completed:
  Current error: 2.45e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 151/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.10e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 81
      Eta min: 2.07e-06

Training for 10000 epochs...

Trial 151 completed:
  Current error: 8.26e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 152/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [31, 31, 31, 31, 31]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.26e-03
  Scheduler: none

Training for 10000 epochs...

Trial 152 completed:
  Current error: 7.02e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 153/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [20, 20]
    Activation: silu
  Optimizers:
    AdamW lr: 6.76e-03
  Scheduler: none

Training for 10000 epochs...

Trial 153 completed:
  Current error: 8.75e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 154/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [78, 78]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.33e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.46
      Patience: 15
      Min lr: 1.26e-05

Training for 10000 epochs...

Trial 154 completed:
  Current error: 2.43e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 155/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.48e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 156
      Eta min: 9.05e-06

Training for 10000 epochs...

Trial 155 completed:
  Current error: 6.92e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 156/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [127, 127]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.81e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 17
      Min lr: 6.99e-05

Training for 10000 epochs...

Trial 156 completed:
  Current error: 1.31e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 157/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [40, 40, 40]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.50e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 95
      Eta min: 6.17e-05

Training for 10000 epochs...

Trial 157 completed:
  Current error: 6.07e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 158/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [64, 64]
    Activation: silu
  Optimizers:
    AdamW lr: 1.01e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 162
      Eta min: 6.25e-05

Training for 10000 epochs...

Trial 158 completed:
  Current error: 1.43e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 159/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [41, 41, 41, 41]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.06e-03
  Scheduler: none

Training for 10000 epochs...

Trial 159 completed:
  Current error: 1.78e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 160/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [115, 115]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.43e-03
  Scheduler: none

Training for 10000 epochs...

Trial 160 completed:
  Current error: 3.60e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 161/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [94, 94, 94, 94]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.60e-03
  Scheduler: none

Training for 10000 epochs...

Trial 161 completed:
  Current error: 5.23e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 162/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [54, 54, 54, 54]
    Activation: silu
  Optimizers:
    AdamW lr: 4.61e-03
  Scheduler: none

Training for 10000 epochs...

Trial 162 completed:
  Current error: 3.40e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 163/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [25, 25, 25, 25]
    Activation: silu
  Optimizers:
    AdamW lr: 9.39e-03
  Scheduler: none

Training for 10000 epochs...

Trial 163 completed:
  Current error: 5.00e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 164/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 20
      Min lr: 5.75e-05

Training for 10000 epochs...

Trial 164 completed:
  Current error: 5.01e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 165/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [113, 113, 113, 113, 113]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: none

Training for 10000 epochs...

Trial 165 completed:
  Current error: 3.85e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 166/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: silu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.44
      Patience: 18
      Min lr: 6.02e-05

Training for 10000 epochs...

Trial 166 completed:
  Current error: 7.38e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 167/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [113, 113]
    Activation: relu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: cosine_annealing
    Parameters:
      T_max: 116
      Eta min: 9.45e-05

Training for 10000 epochs...

Trial 167 completed:
  Current error: 1.00e+00
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 168/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [39, 39, 39, 39]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.92e-03
  Scheduler: none

Training for 10000 epochs...

Trial 168 completed:
  Current error: 5.35e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 169/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [56, 56]
    Activation: silu
  Optimizers:
    AdamW lr: 1.22e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 16
      Min lr: 7.75e-05

Training for 10000 epochs...

Trial 169 completed:
  Current error: 1.65e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 170/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [79, 79, 79, 79]
    Activation: silu
  Optimizers:
    AdamW lr: 8.78e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.36
      Patience: 10
      Min lr: 3.85e-06

Training for 10000 epochs...

Trial 170 completed:
  Current error: 8.92e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 171/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [40, 40, 40, 40]
    Activation: silu
  Optimizers:
    AdamW lr: 2.07e-03
  Scheduler: none

Training for 10000 epochs...

Trial 171 completed:
  Current error: 2.35e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 172/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [55, 55, 55, 55, 55]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.19e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 152
      Eta min: 9.50e-05

Training for 10000 epochs...

Trial 172 completed:
  Current error: 8.66e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 173/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [100, 100]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.07e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.20
      Patience: 11
      Min lr: 9.80e-05

Training for 10000 epochs...

Trial 173 completed:
  Current error: 2.40e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 174/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [33, 33, 33, 33]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.51e-04
  Scheduler: none

Training for 10000 epochs...

Trial 174 completed:
  Current error: 1.04e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 175/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [42, 42, 42]
    Activation: relu
  Optimizers:
    AdamW lr: 8.98e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.29
      Patience: 15
      Min lr: 1.81e-05

Training for 10000 epochs...

Trial 175 completed:
  Current error: 9.99e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 176/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [47, 47]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.85e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.26
      Patience: 7
      Min lr: 4.66e-05

Training for 10000 epochs...

Trial 176 completed:
  Current error: 4.00e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 177/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [43, 43, 43, 43]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.89e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 181
      Eta min: 7.06e-05

Training for 10000 epochs...

Trial 177 completed:
  Current error: 7.99e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 178/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [47, 47]
    Activation: relu
  Optimizers:
    AdamW lr: 8.08e-03
  Scheduler: none

Training for 10000 epochs...

Trial 178 completed:
  Current error: 1.06e+00
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 179/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [20, 20, 20, 20]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.68e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 9
      Min lr: 6.14e-05

Training for 10000 epochs...

Trial 179 completed:
  Current error: 5.73e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 180/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [76, 76, 76, 76]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: cosine_annealing
    Parameters:
      T_max: 50
      Eta min: 3.84e-05

Training for 10000 epochs...

Trial 180 completed:
  Current error: 2.55e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 181/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [52, 52]
    Activation: silu
  Optimizers:
    AdamW lr: 7.86e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 139
      Eta min: 5.91e-05

Training for 10000 epochs...

Trial 181 completed:
  Current error: 1.93e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 182/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [72, 72, 72, 72]
    Activation: relu
  Optimizers:
    AdamW lr: 6.45e-03
  Scheduler: none

Training for 10000 epochs...

Trial 182 completed:
  Current error: 9.99e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 183/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [57, 57, 57]
    Activation: relu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.33
      Patience: 10
      Min lr: 4.54e-05

Training for 10000 epochs...

Trial 183 completed:
  Current error: 1.01e+00
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 184/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [33, 33]
    Activation: silu
  Optimizers:
    AdamW lr: 5.25e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.16
      Patience: 8
      Min lr: 2.92e-05

Training for 10000 epochs...

Trial 184 completed:
  Current error: 3.41e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 185/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [75, 75]
    Activation: relu
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 185 completed:
  Current error: 1.00e+00
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 186/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [41, 41, 41]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.79e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 20
      Min lr: 5.03e-05

Training for 10000 epochs...

Trial 186 completed:
  Current error: 2.16e-01
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 187/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [39, 39, 39, 39]
    Activation: silu
  Optimizers:
    AdamW lr: 3.30e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 115
      Eta min: 8.65e-05

Training for 10000 epochs...

Trial 187 completed:
  Current error: 3.42e-02
  Best error so far: 1.48e-02

------------------------------------------------------------
Trial 188/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [81, 81]
    Activation: silu
  Optimizers:
    AdamW lr: 5.88e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.36e-02
************************************************************

Trial 188 completed:
  Current error: 1.36e-02
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 189/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [127, 127]
    Activation: silu
  Optimizers:
    AdamW lr: 6.15e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.30
      Patience: 10
      Min lr: 6.85e-05

Training for 10000 epochs...

Trial 189 completed:
  Current error: 2.42e-01
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 190/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [34, 34, 34]
    Activation: silu
  Optimizers:
    AdamW lr: 3.55e-03
  Scheduler: none

Training for 10000 epochs...

Trial 190 completed:
  Current error: 4.53e-02
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 191/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [36, 36, 36, 36, 36]
    Activation: relu
  Optimizers:
    AdamW lr: 5.10e-03
  Scheduler: none

Training for 10000 epochs...

Trial 191 completed:
  Current error: 9.99e-01
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 192/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [34, 34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.73e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 104
      Eta min: 8.78e-05

Training for 10000 epochs...

Trial 192 completed:
  Current error: 1.08e-01
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 193/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [102, 102, 102]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.63e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 162
      Eta min: 5.08e-05

Training for 10000 epochs...

Trial 193 completed:
  Current error: 1.07e-01
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 194/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [117, 117, 117]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.90e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 68
      Eta min: 4.86e-05

Training for 10000 epochs...

Trial 194 completed:
  Current error: 3.08e-02
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 195/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [54, 54, 54]
    Activation: relu
  Optimizers:
    AdamW lr: 1.48e-03
  Scheduler: none

Training for 10000 epochs...

Trial 195 completed:
  Current error: 1.00e+00
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 196/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [51, 51]
    Activation: silu
  Optimizers:
    AdamW lr: 2.56e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.40
      Patience: 6
      Min lr: 6.52e-05

Training for 10000 epochs...

Trial 196 completed:
  Current error: 1.09e-01
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 197/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [69, 69, 69]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.20e-03
  Scheduler: none

Training for 10000 epochs...

Trial 197 completed:
  Current error: 2.77e-02
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 198/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [78, 78, 78]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.23e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 88
      Eta min: 3.52e-05

Training for 10000 epochs...

Trial 198 completed:
  Current error: 8.90e-02
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 199/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [105, 105, 105]
    Activation: relu
  Optimizers:
    AdamW lr: 5.88e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 73
      Eta min: 5.90e-05

Training for 10000 epochs...

Trial 199 completed:
  Current error: 1.07e+00
  Best error so far: 1.36e-02

------------------------------------------------------------
Trial 200/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [74, 74, 74]
    Activation: relu
  Optimizers:
    AdamW lr: 2.79e-04
  Scheduler: none

Training for 10000 epochs...

Trial 200 completed:
  Current error: 1.01e+00
  Best error so far: 1.36e-02

============================================================
Optimization completed successfully!
============================================================
Total time: 64839.18 seconds
Best objective value: 1.36e-02

Best parameters found:
------------------------------------------------------------
Architecture:
  Number of layers: 2
  Layer sizes: [81, 81]
  Activation: silu

Optimizers:
  AdamW learning rate: 5.88e-03

Scheduler type: cosine_annealing
  Parameters:
    T_max: 100
    Eta min: 4.37e-05
------------------------------------------------------------

Saving results to logger...
Traceback (most recent call last):
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1305, in <module>
    main(args.nc, args.ni, args.nb, args.nc_test, args.seed, args.epochs)
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1279, in main
    main_with_algorithm(args.algorithm, args.n_trials, args.timeout, algorithm_params=algorithm_params, **params)
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1134, in main_with_algorithm
    results = optimizer.optimize(kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1008, in optimize
    self.logger.log_optimizer_info(
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ResultLogger' object has no attribute 'log_optimizer_info'
[2025-03-08 12:04:59] Successfully completed jade optimization
[2025-03-08 12:05:04] ----------------------------------------
[2025-03-08 12:05:04] Running lshade algorithm
[2025-03-08 12:05:04] ----------------------------------------
[2025-03-08 12:05:04] Starting optimization with lshade algorithm...
/home/user/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

==================================================
Starting SPINN optimization for Klein-Gordon equation
==================================================

Configuration:
Algorithm: LSHADE
NC (collocation points): 1000
NI (initial points): 100
NB (boundary points): 100
NC_TEST (test points): 50
Random seed: 42
Total epochs: 10000
Number of trials: 150
Timeout: None

Results will be saved to: /home/user/SPINN_PyTorch/results/klein_gordon3d/spinn_clear/lshade_20250308_120506
Run parameters saved to run_params.json

============================================================
Initializing LSHADE optimizer
============================================================
Algorithm description:
L-SHADE (Linear Success-History based Adaptive DE) - Улучшенная версия SHADE алгоритма. Использует линейное уменьшение размера популяции и историю успешных решений. Особенно эффективен для задач большой размерности.
------------------------------------------------------------
Configuration:
  Number of trials: 150
  Timeout: None
  Algorithm parameters: {'population_size': 100}
------------------------------------------------------------

Starting optimization process...

============================================================
Starting optimization process
============================================================
Parameter bounds:
  n_layers            : (2, 5)
  layer_size          : (16, 128)
  lr_adamw            : (0.0001, 0.01)
  scheduler_factor    : (0.1, 0.5)
  scheduler_patience  : (5, 20)
  scheduler_min_lr    : (1e-06, 0.0001)
  scheduler_T_max     : (50, 200)
  scheduler_eta_min   : (1e-06, 0.0001)
------------------------------------------------------------

Initializing LSHADE optimizer...

Starting optimization...

------------------------------------------------------------
Trial 1/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: silu
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 2.10e-02
************************************************************

Trial 1 completed:
  Current error: 2.10e-02
  Best error so far: 2.10e-02

------------------------------------------------------------
Trial 2/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: relu
  Optimizers:
    AdamW lr: 3.04e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 17
      Min lr: 2.20e-05

Training for 10000 epochs...

Trial 2 completed:
  Current error: 1.00e+00
  Best error so far: 2.10e-02

------------------------------------------------------------
Trial 3/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: silu
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.88e-02
************************************************************

Trial 3 completed:
  Current error: 1.88e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 4/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [104, 104, 104]
    Activation: relu
  Optimizers:
    AdamW lr: 2.08e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 14
      Min lr: 5.60e-06

Training for 10000 epochs...

Trial 4 completed:
  Current error: 9.99e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 5/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [122, 122]
    Activation: relu
  Optimizers:
    AdamW lr: 9.66e-03
  Scheduler: none

Training for 10000 epochs...

Trial 5 completed:
  Current error: 9.98e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 6/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.40e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 5.25e-05

Training for 10000 epochs...

Trial 6 completed:
  Current error: 3.84e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 7/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [37, 37, 37, 37]
    Activation: silu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.41
      Patience: 19
      Min lr: 8.96e-05

Training for 10000 epochs...

Trial 7 completed:
  Current error: 2.49e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 8/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [38, 38]
    Activation: relu
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: none

Training for 10000 epochs...

Trial 8 completed:
  Current error: 9.99e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 9/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [77, 77, 77]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.50e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.42
      Patience: 6
      Min lr: 9.87e-05

Training for 10000 epochs...

Trial 9 completed:
  Current error: 1.04e-01
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 10/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [107, 107]
    Activation: relu
  Optimizers:
    AdamW lr: 7.10e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.39
      Patience: 17
      Min lr: 8.33e-06

Training for 10000 epochs...

Trial 10 completed:
  Current error: 1.00e+00
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 11/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [86, 86, 86, 86, 86]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 159
      Eta min: 6.41e-05

Training for 10000 epochs...

Trial 11 completed:
  Current error: 7.49e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 12/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [69, 69, 69, 69, 69]
    Activation: relu
  Optimizers:
    AdamW lr: 1.28e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 166
      Eta min: 4.99e-05

Training for 10000 epochs...

Trial 12 completed:
  Current error: 1.00e+00
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 13/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [64, 64, 64, 64]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.52e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.14
      Patience: 5
      Min lr: 6.40e-05

Training for 10000 epochs...

Trial 13 completed:
  Current error: 9.12e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 14/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 14 completed:
  Current error: 4.62e-02
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 15/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: relu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: none

Training for 10000 epochs...

Trial 15 completed:
  Current error: 1.00e+00
  Best error so far: 1.88e-02

------------------------------------------------------------
Trial 16/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [116, 116, 116, 116]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.25e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.72e-02
************************************************************

Trial 16 completed:
  Current error: 1.72e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 17/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [73, 73]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.23e-03
  Scheduler: none

Training for 10000 epochs...

Trial 17 completed:
  Current error: 5.15e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 18/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.70e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 19
      Min lr: 2.59e-05

Training for 10000 epochs...

Trial 18 completed:
  Current error: 1.27e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 19/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [20, 20, 20]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.13e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 186
      Eta min: 2.47e-05

Training for 10000 epochs...

Trial 19 completed:
  Current error: 6.75e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 20/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [71, 71]
    Activation: silu
  Optimizers:
    AdamW lr: 9.86e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.20
      Patience: 15
      Min lr: 7.64e-05

Training for 10000 epochs...

Trial 20 completed:
  Current error: 2.32e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 21/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 98
      Eta min: 1.95e-05

Training for 10000 epochs...

Trial 21 completed:
  Current error: 9.18e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 22/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [82, 82]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.81e-03
  Scheduler: none

Training for 10000 epochs...

Trial 22 completed:
  Current error: 5.79e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 23/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.16
      Patience: 10
      Min lr: 1.22e-05

Training for 10000 epochs...

Trial 23 completed:
  Current error: 5.16e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 24/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [90, 90, 90]
    Activation: relu
  Optimizers:
    AdamW lr: 8.19e-03
  Scheduler: none

Training for 10000 epochs...

Trial 24 completed:
  Current error: 1.01e+00
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 25/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [87, 87, 87, 87, 87]
    Activation: relu
  Optimizers:
    AdamW lr: 3.46e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 183
      Eta min: 7.82e-05

Training for 10000 epochs...

Trial 25 completed:
  Current error: 1.00e+00
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 26/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [25, 25, 25, 25]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.70e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 65
      Eta min: 6.67e-05

Training for 10000 epochs...

Trial 26 completed:
  Current error: 1.08e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 27/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 157
      Eta min: 2.45e-05

Training for 10000 epochs...

Trial 27 completed:
  Current error: 2.93e-02
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 28/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [100, 100, 100]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.44
      Patience: 15
      Min lr: 5.73e-05

Training for 10000 epochs...

Trial 28 completed:
  Current error: 1.09e-01
  Best error so far: 1.72e-02

------------------------------------------------------------
Trial 29/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [43, 43, 43]
    Activation: silu
  Optimizers:
    AdamW lr: 9.73e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.67e-02
************************************************************

Trial 29 completed:
  Current error: 1.67e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 30/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [71, 71, 71, 71]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.03e-03
  Scheduler: none

Training for 10000 epochs...

Trial 30 completed:
  Current error: 4.32e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 31/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [123, 123, 123, 123, 123]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 114
      Eta min: 9.67e-05

Training for 10000 epochs...

Trial 31 completed:
  Current error: 8.30e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 32/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [112, 112, 112, 112, 112]
    Activation: silu
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 18
      Min lr: 3.24e-05

Training for 10000 epochs...

Trial 32 completed:
  Current error: 7.73e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 33/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [94, 94, 94, 94, 94]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.74e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 71
      Eta min: 5.23e-05

Training for 10000 epochs...

Trial 33 completed:
  Current error: 1.98e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 34/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [99, 99, 99, 99, 99]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.00e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 10
      Min lr: 3.01e-05

Training for 10000 epochs...

Trial 34 completed:
  Current error: 7.46e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 35/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [118, 118, 118, 118, 118]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.16e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.30
      Patience: 17
      Min lr: 6.53e-05

Training for 10000 epochs...

Trial 35 completed:
  Current error: 7.07e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 36/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [54, 54, 54, 54, 54]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.82e-03
  Scheduler: none

Training for 10000 epochs...

Trial 36 completed:
  Current error: 2.43e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 37/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [82, 82, 82]
    Activation: relu
  Optimizers:
    AdamW lr: 4.02e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 69
      Eta min: 5.27e-05

Training for 10000 epochs...

Trial 37 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 38/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [40, 40, 40, 40]
    Activation: silu
  Optimizers:
    AdamW lr: 6.27e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 131
      Eta min: 6.41e-05

Training for 10000 epochs...

Trial 38 completed:
  Current error: 3.36e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 39/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [125, 125, 125, 125]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.21e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.23
      Patience: 17
      Min lr: 2.78e-05

Training for 10000 epochs...

Trial 39 completed:
  Current error: 2.33e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 40/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [124, 124]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.38e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 11
      Min lr: 1.82e-05

Training for 10000 epochs...

Trial 40 completed:
  Current error: 3.98e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 41/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [96, 96, 96, 96]
    Activation: silu
  Optimizers:
    AdamW lr: 6.64e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.21
      Patience: 19
      Min lr: 7.41e-05

Training for 10000 epochs...

Trial 41 completed:
  Current error: 9.92e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 42/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [44, 44, 44]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.62e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 57
      Eta min: 5.03e-06

Training for 10000 epochs...

Trial 42 completed:
  Current error: 4.97e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 43/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [95, 95, 95, 95, 95]
    Activation: relu
  Optimizers:
    AdamW lr: 4.79e-03
  Scheduler: none

Training for 10000 epochs...

Trial 43 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 44/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [85, 85, 85]
    Activation: tanh
  Optimizers:
    AdamW lr: 6.39e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.12
      Patience: 11
      Min lr: 6.30e-05

Training for 10000 epochs...

Trial 44 completed:
  Current error: 1.66e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 45/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [34, 34, 34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.99e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.36
      Patience: 5
      Min lr: 5.90e-05

Training for 10000 epochs...

Trial 45 completed:
  Current error: 4.35e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 46/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [88, 88, 88]
    Activation: silu
  Optimizers:
    AdamW lr: 4.64e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 19
      Min lr: 3.92e-05

Training for 10000 epochs...

Trial 46 completed:
  Current error: 6.22e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 47/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [24, 24, 24]
    Activation: relu
  Optimizers:
    AdamW lr: 1.10e-03
  Scheduler: none

Training for 10000 epochs...

Trial 47 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 48/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [19, 19, 19, 19, 19]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 48 completed:
  Current error: 5.73e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 49/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [106, 106, 106, 106]
    Activation: silu
  Optimizers:
    AdamW lr: 2.89e-03
  Scheduler: none

Training for 10000 epochs...

Trial 49 completed:
  Current error: 3.37e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 50/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [103, 103, 103]
    Activation: relu
  Optimizers:
    AdamW lr: 3.47e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 163
      Eta min: 7.57e-05

Training for 10000 epochs...

Trial 50 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 51/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [117, 117]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.10e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.43
      Patience: 10
      Min lr: 8.97e-05

Training for 10000 epochs...

Trial 51 completed:
  Current error: 2.07e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 52/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [26, 26, 26, 26, 26]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.26e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.48
      Patience: 19
      Min lr: 5.78e-05

Training for 10000 epochs...

Trial 52 completed:
  Current error: 4.27e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 53/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [53, 53, 53]
    Activation: relu
  Optimizers:
    AdamW lr: 6.76e-03
  Scheduler: none

Training for 10000 epochs...

Trial 53 completed:
  Current error: 1.01e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 54/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [78, 78]
    Activation: relu
  Optimizers:
    AdamW lr: 4.47e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.46
      Patience: 10
      Min lr: 1.26e-05

Training for 10000 epochs...

Trial 54 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 55/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [27, 27, 27, 27]
    Activation: relu
  Optimizers:
    AdamW lr: 9.33e-04
  Scheduler: none

Training for 10000 epochs...

Trial 55 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 56/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [127, 127]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 198
      Eta min: 7.56e-05

Training for 10000 epochs...

Trial 56 completed:
  Current error: 2.42e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 57/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [25, 25, 25]
    Activation: relu
  Optimizers:
    AdamW lr: 7.79e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 67
      Eta min: 4.98e-05

Training for 10000 epochs...

Trial 57 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 58/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [68, 68]
    Activation: silu
  Optimizers:
    AdamW lr: 6.57e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.15
      Patience: 7
      Min lr: 6.53e-05

Training for 10000 epochs...

Trial 58 completed:
  Current error: 6.23e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 59/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [58, 58, 58, 58, 58]
    Activation: relu
  Optimizers:
    AdamW lr: 2.93e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 52
      Eta min: 9.70e-05

Training for 10000 epochs...

Trial 59 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 60/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [116, 116]
    Activation: relu
  Optimizers:
    AdamW lr: 5.32e-03
  Scheduler: none

Training for 10000 epochs...

Trial 60 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 61/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [94, 94, 94, 94]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.60e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.35
      Patience: 14
      Min lr: 9.02e-05

Training for 10000 epochs...

Trial 61 completed:
  Current error: 1.45e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 62/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [116, 116, 116, 116, 116]
    Activation: silu
  Optimizers:
    AdamW lr: 4.61e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.35
      Patience: 9
      Min lr: 1.96e-05

Training for 10000 epochs...

Trial 62 completed:
  Current error: 1.25e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 63/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [25, 25, 25, 25]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.75e-03
  Scheduler: none

Training for 10000 epochs...

Trial 63 completed:
  Current error: 3.66e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 64/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [34, 34, 34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.12e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 142
      Eta min: 4.24e-05

Training for 10000 epochs...

Trial 64 completed:
  Current error: 2.00e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 65/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [113, 113, 113, 113, 113]
    Activation: relu
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: none

Training for 10000 epochs...

Trial 65 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 66/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: relu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 112
      Eta min: 2.81e-05

Training for 10000 epochs...

Trial 66 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 67/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [113, 113]
    Activation: silu
  Optimizers:
    AdamW lr: 8.15e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.50
      Patience: 20
      Min lr: 5.60e-05

Training for 10000 epochs...

Trial 67 completed:
  Current error: 1.96e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 68/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.56e-03
  Scheduler: none

Training for 10000 epochs...

Trial 68 completed:
  Current error: 2.17e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 69/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [56, 56, 56, 56]
    Activation: silu
  Optimizers:
    AdamW lr: 1.22e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 128
      Eta min: 8.54e-05

Training for 10000 epochs...

Trial 69 completed:
  Current error: 2.98e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 70/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [79, 79, 79, 79]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.78e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.26
      Patience: 7
      Min lr: 3.85e-06

Training for 10000 epochs...

Trial 70 completed:
  Current error: 9.97e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 71/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [40, 40, 40, 40]
    Activation: silu
  Optimizers:
    AdamW lr: 1.45e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 10
      Min lr: 5.94e-05

Training for 10000 epochs...

Trial 71 completed:
  Current error: 1.40e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 72/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [55, 55, 55, 55, 55]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.19e-03
  Scheduler: none

Training for 10000 epochs...

Trial 72 completed:
  Current error: 7.88e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 73/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [120, 120]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.97e-03
  Scheduler: none

Training for 10000 epochs...

Trial 73 completed:
  Current error: 5.71e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 74/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [43, 43, 43, 43]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.51e-04
  Scheduler: none

Training for 10000 epochs...

Trial 74 completed:
  Current error: 6.40e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 75/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [55, 55, 55]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.98e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 79
      Eta min: 5.05e-06

Training for 10000 epochs...

Trial 75 completed:
  Current error: 4.73e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 76/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [47, 47, 47]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.85e-03
  Scheduler: none

Training for 10000 epochs...

Trial 76 completed:
  Current error: 5.94e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 77/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [93, 93, 93, 93]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.89e-04
  Scheduler: none

Training for 10000 epochs...

Trial 77 completed:
  Current error: 5.67e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 78/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [47, 47]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.08e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.40
      Patience: 8
      Min lr: 2.17e-05

Training for 10000 epochs...

Trial 78 completed:
  Current error: 4.02e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 79/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [57, 57, 57, 57]
    Activation: silu
  Optimizers:
    AdamW lr: 4.68e-03
  Scheduler: none

Training for 10000 epochs...

Trial 79 completed:
  Current error: 2.42e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 80/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [76, 76, 76, 76]
    Activation: silu
  Optimizers:
    AdamW lr: 1.16e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 90
      Eta min: 3.84e-05

Training for 10000 epochs...

Trial 80 completed:
  Current error: 5.10e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 81/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [52, 52]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.19e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 139
      Eta min: 6.82e-05

Training for 10000 epochs...

Trial 81 completed:
  Current error: 2.83e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 82/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [72, 72, 72, 72]
    Activation: silu
  Optimizers:
    AdamW lr: 9.61e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 115
      Eta min: 1.36e-05

Training for 10000 epochs...

Trial 82 completed:
  Current error: 4.68e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 83/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [57, 57, 57]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.49e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.33
      Patience: 10
      Min lr: 9.87e-05

Training for 10000 epochs...

Trial 83 completed:
  Current error: 2.28e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 84/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [33, 33]
    Activation: relu
  Optimizers:
    AdamW lr: 2.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.16
      Patience: 8
      Min lr: 2.92e-05

Training for 10000 epochs...

Trial 84 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 85/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [75, 75]
    Activation: relu
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 85 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 86/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [45, 45, 45, 45]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.79e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 136
      Eta min: 2.87e-05

Training for 10000 epochs...

Trial 86 completed:
  Current error: 5.54e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 87/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [37, 37, 37, 37]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.30e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 67
      Eta min: 6.15e-05

Training for 10000 epochs...

Trial 87 completed:
  Current error: 1.91e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 88/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [81, 81, 81]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.63e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.29
      Patience: 13
      Min lr: 6.13e-06

Training for 10000 epochs...

Trial 88 completed:
  Current error: 7.11e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 89/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [127, 127]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.29e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.42
      Patience: 9
      Min lr: 6.85e-05

Training for 10000 epochs...

Trial 89 completed:
  Current error: 2.20e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 90/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [62, 62, 62]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.55e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.47
      Patience: 17
      Min lr: 9.65e-05

Training for 10000 epochs...

Trial 90 completed:
  Current error: 1.11e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 91/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [36, 36, 36, 36, 36]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.58e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.40
      Patience: 14
      Min lr: 8.43e-05

Training for 10000 epochs...

Trial 91 completed:
  Current error: 1.25e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 92/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [34, 34, 34]
    Activation: silu
  Optimizers:
    AdamW lr: 1.73e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 104
      Eta min: 8.78e-05

Training for 10000 epochs...

Trial 92 completed:
  Current error: 1.14e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 93/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [107, 107, 107]
    Activation: relu
  Optimizers:
    AdamW lr: 4.45e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 12
      Min lr: 3.08e-05

Training for 10000 epochs...

Trial 93 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 94/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [117, 117, 117]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.90e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 19
      Min lr: 6.28e-05

Training for 10000 epochs...

Trial 94 completed:
  Current error: 2.02e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 95/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [54, 54, 54, 54]
    Activation: silu
  Optimizers:
    AdamW lr: 1.48e-03
  Scheduler: none

Training for 10000 epochs...

Trial 95 completed:
  Current error: 2.61e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 96/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [51, 51]
    Activation: relu
  Optimizers:
    AdamW lr: 2.56e-03
  Scheduler: none

Training for 10000 epochs...

Trial 96 completed:
  Current error: 1.01e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 97/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [108, 108, 108]
    Activation: relu
  Optimizers:
    AdamW lr: 1.20e-03
  Scheduler: none

Training for 10000 epochs...

Trial 97 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 98/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [97, 97, 97]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.23e-03
  Scheduler: none

Training for 10000 epochs...

Trial 98 completed:
  Current error: 2.88e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 99/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [118, 118, 118]
    Activation: silu
  Optimizers:
    AdamW lr: 5.88e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.26
      Patience: 12
      Min lr: 9.48e-05

Training for 10000 epochs...

Trial 99 completed:
  Current error: 1.28e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 100/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [84, 84, 84, 84]
    Activation: tanh
  Optimizers:
    AdamW lr: 2.79e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 154
      Eta min: 9.23e-05

Training for 10000 epochs...

Trial 100 completed:
  Current error: 8.57e-02
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 101/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.34
      Patience: 7
      Min lr: 2.95e-05

Training for 10000 epochs...

Trial 101 completed:
  Current error: 2.69e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 102/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: silu
  Optimizers:
    AdamW lr: 3.04e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.49
      Patience: 17
      Min lr: 2.20e-05

Training for 10000 epochs...

Trial 102 completed:
  Current error: 1.07e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 103/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: relu
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: none

Training for 10000 epochs...

Trial 103 completed:
  Current error: 1.00e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 104/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [72, 72, 72]
    Activation: silu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 14
      Min lr: 2.49e-05

Training for 10000 epochs...

Trial 104 completed:
  Current error: 3.75e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 105/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [54, 54]
    Activation: relu
  Optimizers:
    AdamW lr: 7.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 13
      Min lr: 1.07e-05

Training for 10000 epochs...

Trial 105 completed:
  Current error: 9.99e-01
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 106/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [71, 71, 71]
    Activation: relu
  Optimizers:
    AdamW lr: 3.50e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.19
      Patience: 9
      Min lr: 6.66e-05

Training for 10000 epochs...

Trial 106 completed:
  Current error: 1.01e+00
  Best error so far: 1.67e-02

------------------------------------------------------------
Trial 107/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [37, 37, 37, 37]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.43e-02
************************************************************

Trial 107 completed:
  Current error: 1.43e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 108/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [38, 38]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: none

Training for 10000 epochs...

Trial 108 completed:
  Current error: 4.18e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 109/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [63, 63, 63]
    Activation: relu
  Optimizers:
    AdamW lr: 1.50e-03
  Scheduler: none

Training for 10000 epochs...

Trial 109 completed:
  Current error: 1.00e+00
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 110/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [92, 92, 92]
    Activation: relu
  Optimizers:
    AdamW lr: 7.10e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 95
      Eta min: 1.25e-05

Training for 10000 epochs...

Trial 110 completed:
  Current error: 9.99e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 111/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [86, 86, 86, 86, 86]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 154
      Eta min: 6.41e-05

Training for 10000 epochs...

Trial 111 completed:
  Current error: 2.59e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 112/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [69, 69, 69, 69, 69]
    Activation: relu
  Optimizers:
    AdamW lr: 2.96e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 166
      Eta min: 4.99e-05

Training for 10000 epochs...

Trial 112 completed:
  Current error: 1.00e+00
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 113/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [64, 64, 64, 64]
    Activation: silu
  Optimizers:
    AdamW lr: 3.52e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 97
      Eta min: 5.13e-05

Training for 10000 epochs...

Trial 113 completed:
  Current error: 3.73e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 114/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: relu
  Optimizers:
    AdamW lr: 8.00e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 78
      Eta min: 6.98e-05

Training for 10000 epochs...

Trial 114 completed:
  Current error: 9.23e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 115/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [107, 107, 107, 107, 107]
    Activation: silu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 20
      Min lr: 1.95e-05

Training for 10000 epochs...

Trial 115 completed:
  Current error: 1.85e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 116/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [116, 116, 116, 116]
    Activation: relu
  Optimizers:
    AdamW lr: 9.01e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.15
      Patience: 8
      Min lr: 4.33e-05

Training for 10000 epochs...

Trial 116 completed:
  Current error: 1.01e+00
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 117/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.23e-03
  Scheduler: none

Training for 10000 epochs...

Trial 117 completed:
  Current error: 9.06e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 118/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [95, 95, 95, 95]
    Activation: relu
  Optimizers:
    AdamW lr: 7.81e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 125
      Eta min: 4.55e-05

Training for 10000 epochs...

Trial 118 completed:
  Current error: 1.01e+00
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 119/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [50, 50, 50]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.13e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.30
      Patience: 17
      Min lr: 5.65e-05

Training for 10000 epochs...

Trial 119 completed:
  Current error: 1.56e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 120/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [71, 71, 71, 71]
    Activation: relu
  Optimizers:
    AdamW lr: 9.49e-03
  Scheduler: none

Training for 10000 epochs...

Trial 120 completed:
  Current error: 1.00e+00
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 121/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [87, 87, 87]
    Activation: silu
  Optimizers:
    AdamW lr: 6.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 6
      Min lr: 1.00e-04

Training for 10000 epochs...

Trial 121 completed:
  Current error: 1.27e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 122/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [82, 82]
    Activation: silu
  Optimizers:
    AdamW lr: 6.02e-03
  Scheduler: none

Training for 10000 epochs...

Trial 122 completed:
  Current error: 1.84e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 123/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 9.37e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.16
      Patience: 10
      Min lr: 1.22e-05

Training for 10000 epochs...

Trial 123 completed:
  Current error: 9.99e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 124/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [90, 90, 90]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.36e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.32
      Patience: 13
      Min lr: 2.49e-05

Training for 10000 epochs...

Trial 124 completed:
  Current error: 4.10e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 125/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [95, 95, 95]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.46e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 16
      Min lr: 8.98e-05

Training for 10000 epochs...

Trial 125 completed:
  Current error: 8.15e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 126/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [25, 25]
    Activation: relu
  Optimizers:
    AdamW lr: 1.70e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 87
      Eta min: 9.01e-05

Training for 10000 epochs...

Trial 126 completed:
  Current error: 9.98e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 127/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [26, 26]
    Activation: relu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 157
      Eta min: 2.45e-05

Training for 10000 epochs...

Trial 127 completed:
  Current error: 1.00e+00
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 128/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.38e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 117
      Eta min: 7.36e-05

Training for 10000 epochs...

Trial 128 completed:
  Current error: 1.71e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 129/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [51, 51, 51]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.12e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 169
      Eta min: 5.08e-05

Training for 10000 epochs...

Trial 129 completed:
  Current error: 6.21e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 130/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [71, 71, 71, 71]
    Activation: silu
  Optimizers:
    AdamW lr: 6.01e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 147
      Eta min: 1.85e-05

Training for 10000 epochs...

Trial 130 completed:
  Current error: 2.05e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 131/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [123, 123, 123, 123]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.34e-03
  Scheduler: none

Training for 10000 epochs...

Trial 131 completed:
  Current error: 6.54e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 132/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [112, 112, 112, 112]
    Activation: silu
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 18
      Min lr: 3.24e-05

Training for 10000 epochs...

Trial 132 completed:
  Current error: 8.02e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 133/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [85, 85, 85, 85]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.74e-03
  Scheduler: none

Training for 10000 epochs...

Trial 133 completed:
  Current error: 4.77e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 134/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [99, 99, 99, 99, 99]
    Activation: silu
  Optimizers:
    AdamW lr: 7.00e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.36
      Patience: 10
      Min lr: 4.28e-05

Training for 10000 epochs...

Trial 134 completed:
  Current error: 1.14e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 135/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [108, 108, 108, 108]
    Activation: relu
  Optimizers:
    AdamW lr: 4.32e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.28
      Patience: 14
      Min lr: 6.53e-05

Training for 10000 epochs...

Trial 135 completed:
  Current error: 9.99e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 136/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 9.33e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.10
      Patience: 20
      Min lr: 9.97e-05

Training for 10000 epochs...

Trial 136 completed:
  Current error: 1.91e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 137/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [37, 37, 37]
    Activation: relu
  Optimizers:
    AdamW lr: 4.02e-04
  Scheduler: none

Training for 10000 epochs...

Trial 137 completed:
  Current error: 1.00e+00
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 138/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [40, 40, 40, 40]
    Activation: silu
  Optimizers:
    AdamW lr: 6.24e-03
  Scheduler: none

Training for 10000 epochs...

Trial 138 completed:
  Current error: 3.30e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 139/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [125, 125, 125, 125]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.21e-03
  Scheduler: none

Training for 10000 epochs...

Trial 139 completed:
  Current error: 3.33e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 140/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [124, 124]
    Activation: relu
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: cosine_annealing
    Parameters:
      T_max: 73
      Eta min: 1.00e-04

Training for 10000 epochs...

Trial 140 completed:
  Current error: 1.04e+00
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 141/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [80, 80, 80, 80]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.35e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.21
      Patience: 15
      Min lr: 6.90e-05

Training for 10000 epochs...

Trial 141 completed:
  Current error: 1.94e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 142/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [56, 56, 56]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.62e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.41
      Patience: 5
      Min lr: 3.53e-05

Training for 10000 epochs...

Trial 142 completed:
  Current error: 1.82e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 143/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [95, 95, 95]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.49e-03
  Scheduler: none

Training for 10000 epochs...

Trial 143 completed:
  Current error: 1.61e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 144/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [85, 85, 85]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.49e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 125
      Eta min: 8.58e-05

Training for 10000 epochs...

Trial 144 completed:
  Current error: 2.22e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 145/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [36, 36, 36, 36]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.47e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.36
      Patience: 18
      Min lr: 5.90e-05

Training for 10000 epochs...

Trial 145 completed:
  Current error: 3.23e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 146/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [58, 58, 58]
    Activation: silu
  Optimizers:
    AdamW lr: 4.64e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.17
      Patience: 18
      Min lr: 3.92e-05

Training for 10000 epochs...

Trial 146 completed:
  Current error: 1.02e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 147/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [66, 66, 66]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.67e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.10
      Patience: 15
      Min lr: 6.86e-05

Training for 10000 epochs...

Trial 147 completed:
  Current error: 1.63e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 148/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [19, 19, 19, 19, 19]
    Activation: silu
  Optimizers:
    AdamW lr: 8.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 148 completed:
  Current error: 1.82e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 149/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [106, 106, 106, 106]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.14e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 199
      Eta min: 6.77e-05

Training for 10000 epochs...

Trial 149 completed:
  Current error: 2.97e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 150/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [101, 101, 101, 101]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.20e-03
  Scheduler: none

Training for 10000 epochs...

Trial 150 completed:
  Current error: 1.74e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 151/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [117, 117]
    Activation: silu
  Optimizers:
    AdamW lr: 4.57e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 108
      Eta min: 2.07e-06

Training for 10000 epochs...

Trial 151 completed:
  Current error: 2.30e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 152/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.84e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.48
      Patience: 19
      Min lr: 5.78e-05

Training for 10000 epochs...

Trial 152 completed:
  Current error: 6.97e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 153/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [17, 17]
    Activation: silu
  Optimizers:
    AdamW lr: 6.76e-03
  Scheduler: none

Training for 10000 epochs...

Trial 153 completed:
  Current error: 2.32e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 154/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [78, 78, 78, 78]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.90e-03
  Scheduler: none

Training for 10000 epochs...

Trial 154 completed:
  Current error: 1.82e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 155/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [31, 31, 31, 31]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.36e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 156
      Eta min: 9.05e-06

Training for 10000 epochs...

Trial 155 completed:
  Current error: 3.57e-02
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 156/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [127, 127]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.81e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 17
      Min lr: 7.48e-05

Training for 10000 epochs...

Trial 156 completed:
  Current error: 1.07e-01
  Best error so far: 1.43e-02

------------------------------------------------------------
Trial 157/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [38, 38, 38]
    Activation: silu
  Optimizers:
    AdamW lr: 7.79e-03
  Scheduler: none

Training for 10000 epochs...

************************************************************
New best error found: 1.23e-02
************************************************************

Trial 157 completed:
  Current error: 1.23e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 158/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [64, 64]
    Activation: silu
  Optimizers:
    AdamW lr: 2.04e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 162
      Eta min: 4.66e-05

Training for 10000 epochs...

Trial 158 completed:
  Current error: 2.64e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 159/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [38, 38, 38, 38]
    Activation: gelu
  Optimizers:
    AdamW lr: 6.94e-03
  Scheduler: none

Training for 10000 epochs...

Trial 159 completed:
  Current error: 4.78e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 160/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [112, 112]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.55e-03
  Scheduler: none

Training for 10000 epochs...

Trial 160 completed:
  Current error: 4.76e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 161/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [39, 39, 39]
    Activation: relu
  Optimizers:
    AdamW lr: 9.06e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.50
      Patience: 14
      Min lr: 1.00e-04

Training for 10000 epochs...

Trial 161 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 162/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [116, 116, 116, 116, 116]
    Activation: silu
  Optimizers:
    AdamW lr: 5.83e-03
  Scheduler: none

Training for 10000 epochs...

Trial 162 completed:
  Current error: 2.07e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 163/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: relu
  Optimizers:
    AdamW lr: 9.75e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 50
      Eta min: 8.16e-05

Training for 10000 epochs...

Trial 163 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 164/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [20, 20, 20, 20]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.12e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 142
      Eta min: 7.30e-05

Training for 10000 epochs...

Trial 164 completed:
  Current error: 8.68e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 165/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [113, 113, 113, 113, 113]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.48e-04
  Scheduler: none

Training for 10000 epochs...

Trial 165 completed:
  Current error: 3.65e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 166/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [59, 59, 59, 59]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.39
      Patience: 15
      Min lr: 4.74e-05

Training for 10000 epochs...

Trial 166 completed:
  Current error: 5.41e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 167/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [113, 113]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.88e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 113
      Eta min: 9.45e-05

Training for 10000 epochs...

Trial 167 completed:
  Current error: 9.47e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 168/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [44, 44, 44, 44]
    Activation: relu
  Optimizers:
    AdamW lr: 5.99e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.35
      Patience: 13
      Min lr: 7.98e-05

Training for 10000 epochs...

Trial 168 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 169/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [44, 44, 44, 44]
    Activation: silu
  Optimizers:
    AdamW lr: 1.22e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 13
      Min lr: 6.29e-05

Training for 10000 epochs...

Trial 169 completed:
  Current error: 1.36e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 170/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [79, 79, 79, 79]
    Activation: silu
  Optimizers:
    AdamW lr: 8.78e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.35
      Patience: 8
      Min lr: 3.85e-06

Training for 10000 epochs...

Trial 170 completed:
  Current error: 9.30e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 171/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [40, 40, 40, 40]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.45e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.10
      Patience: 9
      Min lr: 9.84e-05

Training for 10000 epochs...

Trial 171 completed:
  Current error: 1.07e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 172/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [55, 55, 55, 55, 55]
    Activation: silu
  Optimizers:
    AdamW lr: 7.84e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 179
      Eta min: 8.20e-05

Training for 10000 epochs...

Trial 172 completed:
  Current error: 3.57e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 173/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [82, 82, 82]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.97e-03
  Scheduler: none

Training for 10000 epochs...

Trial 173 completed:
  Current error: 4.84e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 174/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [43, 43, 43, 43]
    Activation: relu
  Optimizers:
    AdamW lr: 8.51e-04
  Scheduler: none

Training for 10000 epochs...

Trial 174 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 175/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [46, 46, 46]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.98e-03
  Scheduler: none

Training for 10000 epochs...

Trial 175 completed:
  Current error: 2.35e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 176/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [57, 57, 57]
    Activation: silu
  Optimizers:
    AdamW lr: 1.85e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 115
      Eta min: 3.71e-05

Training for 10000 epochs...

Trial 176 completed:
  Current error: 3.86e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 177/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [93, 93]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.89e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 14
      Min lr: 9.09e-06

Training for 10000 epochs...

Trial 177 completed:
  Current error: 8.70e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 178/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [17, 17]
    Activation: gelu
  Optimizers:
    AdamW lr: 8.08e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 106
      Eta min: 7.50e-05

Training for 10000 epochs...

Trial 178 completed:
  Current error: 6.73e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 179/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [45, 45, 45, 45]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.24e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 138
      Eta min: 8.96e-05

Training for 10000 epochs...

Trial 179 completed:
  Current error: 3.24e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 180/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 180 completed:
  Current error: 1.48e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 181/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [52, 52]
    Activation: relu
  Optimizers:
    AdamW lr: 2.19e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 116
      Eta min: 7.20e-05

Training for 10000 epochs...

Trial 181 completed:
  Current error: 9.98e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 182/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [72, 72, 72, 72]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.61e-04
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.31
      Patience: 9
      Min lr: 1.00e-04

Training for 10000 epochs...

Trial 182 completed:
  Current error: 7.84e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 183/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [29, 29]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.54e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.20
      Patience: 5
      Min lr: 1.00e-04

Training for 10000 epochs...

Trial 183 completed:
  Current error: 1.60e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 184/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [33, 33, 33]
    Activation: tanh
  Optimizers:
    AdamW lr: 2.53e-03
  Scheduler: none

Training for 10000 epochs...

Trial 184 completed:
  Current error: 5.00e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 185/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [75, 75]
    Activation: silu
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 185 completed:
  Current error: 1.61e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 186/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [24, 24, 24, 24]
    Activation: relu
  Optimizers:
    AdamW lr: 1.79e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.27
      Patience: 19
      Min lr: 4.50e-05

Training for 10000 epochs...

Trial 186 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 187/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.30e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 13
      Min lr: 8.94e-05

Training for 10000 epochs...

Trial 187 completed:
  Current error: 7.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 188/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [81, 81, 81, 81]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.63e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.29
      Patience: 10
      Min lr: 5.28e-05

Training for 10000 epochs...

Trial 188 completed:
  Current error: 1.01e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 189/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [43, 43]
    Activation: silu
  Optimizers:
    AdamW lr: 7.88e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.36
      Patience: 9
      Min lr: 5.89e-05

Training for 10000 epochs...

Trial 189 completed:
  Current error: 3.61e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 190/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [62, 62, 62]
    Activation: gelu
  Optimizers:
    AdamW lr: 3.55e-03
  Scheduler: none

Training for 10000 epochs...

Trial 190 completed:
  Current error: 2.20e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 191/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [36, 36, 36, 36, 36]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.03e-03
  Scheduler: none

Training for 10000 epochs...

Trial 191 completed:
  Current error: 5.90e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 192/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [34, 34, 34]
    Activation: relu
  Optimizers:
    AdamW lr: 3.55e-03
  Scheduler: none

Training for 10000 epochs...

Trial 192 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 193/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [107, 107, 107]
    Activation: relu
  Optimizers:
    AdamW lr: 5.43e-03
  Scheduler: none

Training for 10000 epochs...

Trial 193 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 194/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [117, 117]
    Activation: silu
  Optimizers:
    AdamW lr: 5.96e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 75
      Eta min: 9.40e-05

Training for 10000 epochs...

Trial 194 completed:
  Current error: 2.55e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 195/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [54, 54, 54, 54]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.48e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 184
      Eta min: 6.19e-05

Training for 10000 epochs...

Trial 195 completed:
  Current error: 4.30e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 196/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [52, 52]
    Activation: relu
  Optimizers:
    AdamW lr: 2.56e-03
  Scheduler: none

Training for 10000 epochs...

Trial 196 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 197/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [108, 108, 108]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.20e-03
  Scheduler: none

Training for 10000 epochs...

Trial 197 completed:
  Current error: 4.54e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 198/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [76, 76, 76]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.23e-03
  Scheduler: none

Training for 10000 epochs...

Trial 198 completed:
  Current error: 8.45e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 199/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [67, 67, 67]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.88e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 73
      Eta min: 5.90e-05

Training for 10000 epochs...

Trial 199 completed:
  Current error: 5.66e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 200/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [66, 66, 66, 66]
    Activation: relu
  Optimizers:
    AdamW lr: 2.79e-04
  Scheduler: cosine_annealing
    Parameters:
      T_max: 154
      Eta min: 9.23e-05

Training for 10000 epochs...

Trial 200 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 201/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [29, 29, 29]
    Activation: relu
  Optimizers:
    AdamW lr: 7.79e-03
  Scheduler: none

Training for 10000 epochs...

Trial 201 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 202/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [37, 37, 37, 37]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.49e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.25
      Patience: 20
      Min lr: 8.54e-05

Training for 10000 epochs...

Trial 202 completed:
  Current error: 2.48e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 203/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [75, 75]
    Activation: relu
  Optimizers:
    AdamW lr: 3.78e-03
  Scheduler: none

Training for 10000 epochs...

Trial 203 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 204/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [43, 43, 43]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.70e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 116
      Eta min: 5.85e-05

Training for 10000 epochs...

Trial 204 completed:
  Current error: 2.26e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 205/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [75, 75, 75]
    Activation: tanh
  Optimizers:
    AdamW lr: 7.38e-03
  Scheduler: none

Training for 10000 epochs...

Trial 205 completed:
  Current error: 7.64e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 206/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [116, 116, 116, 116]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.25e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.10
      Patience: 8
      Min lr: 9.17e-05

Training for 10000 epochs...

Trial 206 completed:
  Current error: 1.28e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 207/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [54, 54, 54]
    Activation: relu
  Optimizers:
    AdamW lr: 7.95e-03
  Scheduler: none

Training for 10000 epochs...

Trial 207 completed:
  Current error: 1.01e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 208/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [19, 19, 19, 19]
    Activation: tanh
  Optimizers:
    AdamW lr: 8.63e-03
  Scheduler: none

Training for 10000 epochs...

Trial 208 completed:
  Current error: 1.04e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 209/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [82, 82, 82]
    Activation: relu
  Optimizers:
    AdamW lr: 6.30e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.11
      Patience: 10
      Min lr: 8.89e-05

Training for 10000 epochs...

Trial 209 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 210/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [78, 78, 78]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.38e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.15
      Patience: 9
      Min lr: 5.81e-05

Training for 10000 epochs...

Trial 210 completed:
  Current error: 1.10e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 211/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.30e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 52
      Eta min: 6.15e-05

Training for 10000 epochs...

Trial 211 completed:
  Current error: 7.13e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 212/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [94, 94, 94, 94, 94]
    Activation: relu
  Optimizers:
    AdamW lr: 5.74e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 99
      Eta min: 5.23e-05

Training for 10000 epochs...

Trial 212 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 213/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [34, 34, 34]
    Activation: gelu
  Optimizers:
    AdamW lr: 9.83e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 142
      Eta min: 6.98e-05

Training for 10000 epochs...

Trial 213 completed:
  Current error: 2.82e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 214/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [66, 66, 66, 66]
    Activation: silu
  Optimizers:
    AdamW lr: 2.41e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 91
      Eta min: 1.85e-05

Training for 10000 epochs...

Trial 214 completed:
  Current error: 2.74e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 215/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [116, 116, 116, 116, 116]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.83e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 80
      Eta min: 7.77e-05

Training for 10000 epochs...

Trial 215 completed:
  Current error: 9.62e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 216/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [122, 122, 122]
    Activation: silu
  Optimizers:
    AdamW lr: 7.35e-03
  Scheduler: none

Training for 10000 epochs...

Trial 216 completed:
  Current error: 6.53e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 217/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [19, 19, 19, 19, 19]
    Activation: silu
  Optimizers:
    AdamW lr: 4.56e-03
  Scheduler: none

Training for 10000 epochs...

Trial 217 completed:
  Current error: 4.82e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 218/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [102, 102, 102]
    Activation: relu
  Optimizers:
    AdamW lr: 3.55e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 109
      Eta min: 7.49e-05

Training for 10000 epochs...

Trial 218 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 219/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [85, 85, 85]
    Activation: relu
  Optimizers:
    AdamW lr: 6.39e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.10
      Patience: 17
      Min lr: 5.74e-05

Training for 10000 epochs...

Trial 219 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 220/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [117, 117]
    Activation: tanh
  Optimizers:
    AdamW lr: 9.81e-03
  Scheduler: none

Training for 10000 epochs...

Trial 220 completed:
  Current error: 1.94e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 221/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [46, 46, 46]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.00e-02
  Scheduler: none

Training for 10000 epochs...

Trial 221 completed:
  Current error: 7.17e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 222/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [55, 55]
    Activation: silu
  Optimizers:
    AdamW lr: 7.40e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 144
      Eta min: 1.00e-04

Training for 10000 epochs...

Trial 222 completed:
  Current error: 1.78e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 223/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [62, 62, 62, 62]
    Activation: tanh
  Optimizers:
    AdamW lr: 4.18e-03
  Scheduler: none

Training for 10000 epochs...

Trial 223 completed:
  Current error: 4.33e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 224/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [54, 54, 54, 54]
    Activation: relu
  Optimizers:
    AdamW lr: 5.91e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.12
      Patience: 10
      Min lr: 4.56e-06

Training for 10000 epochs...

Trial 224 completed:
  Current error: 9.98e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 225/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [117, 117, 117]
    Activation: tanh
  Optimizers:
    AdamW lr: 5.96e-03
  Scheduler: none

Training for 10000 epochs...

Trial 225 completed:
  Current error: 6.99e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 226/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: silu
  Optimizers:
    AdamW lr: 3.38e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.14
      Patience: 12
      Min lr: 6.35e-05

Training for 10000 epochs...

Trial 226 completed:
  Current error: 7.25e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 227/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [67, 67, 67, 67]
    Activation: tanh
  Optimizers:
    AdamW lr: 1.48e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.42
      Patience: 10
      Min lr: 5.38e-05

Training for 10000 epochs...

Trial 227 completed:
  Current error: 1.31e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 228/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [64, 64]
    Activation: gelu
  Optimizers:
    AdamW lr: 2.04e-03
  Scheduler: none

Training for 10000 epochs...

Trial 228 completed:
  Current error: 6.69e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 229/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [52, 52]
    Activation: gelu
  Optimizers:
    AdamW lr: 4.87e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.17
      Patience: 7
      Min lr: 8.92e-05

Training for 10000 epochs...

Trial 229 completed:
  Current error: 2.65e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 230/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [59, 59, 59]
    Activation: relu
  Optimizers:
    AdamW lr: 7.23e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.26
      Patience: 16
      Min lr: 5.47e-05

Training for 10000 epochs...

Trial 230 completed:
  Current error: 1.01e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 231/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [34, 34]
    Activation: relu
  Optimizers:
    AdamW lr: 5.53e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 14
      Min lr: 2.32e-05

Training for 10000 epochs...

Trial 231 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 232/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [106, 106, 106, 106]
    Activation: relu
  Optimizers:
    AdamW lr: 6.14e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 128
      Eta min: 6.77e-05

Training for 10000 epochs...

Trial 232 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 233/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [56, 56, 56, 56]
    Activation: silu
  Optimizers:
    AdamW lr: 1.22e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.37
      Patience: 13
      Min lr: 7.75e-05

Training for 10000 epochs...

Trial 233 completed:
  Current error: 9.00e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 234/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [40, 40, 40, 40, 40]
    Activation: relu
  Optimizers:
    AdamW lr: 6.56e-03
  Scheduler: none

Training for 10000 epochs...

Trial 234 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 235/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [125, 125, 125]
    Activation: tanh
  Optimizers:
    AdamW lr: 3.98e-03
  Scheduler: none

Training for 10000 epochs...

Trial 235 completed:
  Current error: 7.55e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 236/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [55, 55, 55, 55, 55]
    Activation: relu
  Optimizers:
    AdamW lr: 7.09e-03
  Scheduler: none

Training for 10000 epochs...

Trial 236 completed:
  Current error: 1.00e+00
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 237/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [31, 31, 31, 31]
    Activation: relu
  Optimizers:
    AdamW lr: 3.02e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 11
      Min lr: 8.24e-05

Training for 10000 epochs...

Trial 237 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 238/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [62, 62, 62, 62, 62]
    Activation: gelu
  Optimizers:
    AdamW lr: 5.52e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 198
      Eta min: 1.59e-05

Training for 10000 epochs...

Trial 238 completed:
  Current error: 1.69e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 239/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [33, 33, 33, 33]
    Activation: relu
  Optimizers:
    AdamW lr: 9.75e-03
  Scheduler: none

Training for 10000 epochs...

Trial 239 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 240/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [16, 16, 16, 16]
    Activation: gelu
  Optimizers:
    AdamW lr: 7.63e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 62
      Eta min: 5.13e-05

Training for 10000 epochs...

Trial 240 completed:
  Current error: 4.61e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 241/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 2
    Layer sizes: [85, 85]
    Activation: relu
  Optimizers:
    AdamW lr: 4.40e-04
  Scheduler: none

Training for 10000 epochs...

Trial 241 completed:
  Current error: 9.98e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 242/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [100, 100, 100, 100]
    Activation: silu
  Optimizers:
    AdamW lr: 1.85e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.14
      Patience: 6
      Min lr: 1.00e-04

Training for 10000 epochs...

Trial 242 completed:
  Current error: 6.52e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 243/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [16, 16, 16]
    Activation: silu
  Optimizers:
    AdamW lr: 2.71e-03
  Scheduler: none

Training for 10000 epochs...

Trial 243 completed:
  Current error: 4.43e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 244/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [108, 108, 108]
    Activation: gelu
  Optimizers:
    AdamW lr: 1.20e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 168
      Eta min: 1.61e-05

Training for 10000 epochs...

Trial 244 completed:
  Current error: 4.56e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 245/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [44, 44, 44, 44, 44]
    Activation: relu
  Optimizers:
    AdamW lr: 4.16e-03
  Scheduler: none

Training for 10000 epochs...

Trial 245 completed:
  Current error: 9.98e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 246/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 4
    Layer sizes: [72, 72, 72, 72]
    Activation: silu
  Optimizers:
    AdamW lr: 1.49e-03
  Scheduler: none

Training for 10000 epochs...

Trial 246 completed:
  Current error: 3.52e-02
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 247/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [91, 91, 91]
    Activation: silu
  Optimizers:
    AdamW lr: 8.55e-03
  Scheduler: reduce_on_plateau
    Parameters:
      Factor: 0.38
      Patience: 12
      Min lr: 5.58e-05

Training for 10000 epochs...

Trial 247 completed:
  Current error: 2.16e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 248/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 5
    Layer sizes: [38, 38, 38, 38, 38]
    Activation: relu
  Optimizers:
    AdamW lr: 6.94e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 93
      Eta min: 7.74e-05

Training for 10000 epochs...

Trial 248 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 249/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [82, 82, 82]
    Activation: relu
  Optimizers:
    AdamW lr: 5.00e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 166
      Eta min: 3.35e-05

Training for 10000 epochs...

Trial 249 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

------------------------------------------------------------
Trial 250/150
------------------------------------------------------------
Current parameters:
  Architecture:
    Layers: 3
    Layer sizes: [67, 67, 67]
    Activation: relu
  Optimizers:
    AdamW lr: 3.62e-03
  Scheduler: cosine_annealing
    Parameters:
      T_max: 77
      Eta min: 5.03e-06

Training for 10000 epochs...

Trial 250 completed:
  Current error: 9.99e-01
  Best error so far: 1.23e-02

============================================================
Optimization completed successfully!
============================================================
Total time: 81277.32 seconds
Best objective value: 1.23e-02

Best parameters found:
------------------------------------------------------------
Architecture:
  Number of layers: 3
  Layer sizes: [38, 38, 38]
  Activation: gelu

Optimizers:
  AdamW learning rate: 7.79e-03

Scheduler type: none
------------------------------------------------------------

Saving results to logger...
Traceback (most recent call last):
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1305, in <module>
    main(args.nc, args.ni, args.nb, args.nc_test, args.seed, args.epochs)
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1279, in main
    main_with_algorithm(args.algorithm, args.n_trials, args.timeout, algorithm_params=algorithm_params, **params)
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1134, in main_with_algorithm
    results = optimizer.optimize(kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/SPINN_PyTorch/demo/demo_spinn_pytorch_my_exps.py", line 1008, in optimize
    self.logger.log_optimizer_info(
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ResultLogger' object has no attribute 'log_optimizer_info'
[2025-03-09 10:39:45] Successfully completed lshade optimization
[2025-03-09 10:39:50] ----------------------------------------
[2025-03-09 10:39:50] All algorithms completed successfully
[2025-03-09 10:39:50] ----------------------------------------
